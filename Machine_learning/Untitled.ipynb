{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e3cb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset, DatasetDict\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorForTokenClassification\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "print('starting!')\n",
    "import time\n",
    "total_start = time.time()\n",
    "import sys\n",
    "import json\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "print('after transformers imports, time:', time.time() - total_start)\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import pickle\n",
    "import concurrent.futures\n",
    "import csv\n",
    "\n",
    "USE_ONNX = False\n",
    "\n",
    "if USE_ONNX:\n",
    "    sys.path.append('/kaggle/input/python-packages/python_packages')\n",
    "    import onnxruntime\n",
    "\n",
    "print('after all imports (inc. torch), time:', time.time() - total_start)\n",
    "\n",
    "NAMES_SELECTION = 1 #1, 2 or None - change this to use different names lists or not to use names list at all\n",
    "\n",
    "# data_filepath = 'data/test.json'\n",
    "# data_filepath = 'data/train.json'\n",
    "data_filepath = '/kaggle/input/pii-detection-removal-from-educational-data/test.json'\n",
    "# data_filepath = '/kaggle/input/pii-detection-removal-from-educational-data/train.json'\n",
    "\n",
    "base_model = 'microsoft/mdeberta-v3-base'\n",
    "inference_model_name = '/kaggle/input/xs2-ft3ds-3e-l64-2889/xs2_ft3ds_on_3dscheckpoint-29630_len64_3e_gelu_checkpoint-2889'\n",
    "inference_model_name2 = '/kaggle/input/xs-ft3ds-2e-l64-1926/xs_ft3ds_on_903ds_2e_0.928_len64_gelu_checkpoint-1926'\n",
    "\n",
    "### other models\n",
    "# inference_model_name2 = '/kaggle/input/92-2-ft-on-3ds-xsmall/92_ft_on_3ds_xsmall_checkpoint-3059' # older, better performing model\n",
    "# inference_model_name2 = '/kaggle/input/nimp2_cni1/pytorch/nimp3/1/checkpoint-34884'               # single-phase training\n",
    "# inference_model_name2 = '/kaggle/input/nimp2_cni1/pytorch/nimo6_c2/1/checkpoint-1926'             # strong and easy to train\n",
    "# inference_model_name2 = '/kaggle/input/nimp2_cni1/pytorch/conly_e3/1/checkpoint-2889'             # only competition data\n",
    "# inference_model_name = '/kaggle/input/nimp2_cni1/pytorch/conly_xs2_e3/1/checkpoint-2889'          # only competition data smaller model\n",
    "# inference_model_name = '/kaggle/input/nimp2_cni1/pytorch/xs2_nimo5_c2/1/checkpoint-1926'          # strong and easy to train smaller model\n",
    "###\n",
    "base_model = inference_model_name\n",
    "\n",
    "onnx_model20 = '/kaggle/input/onnx-l64-q-o3/xs_len64_onnx_q_o3/model_quantized_optimized.onnx' # based on: xs_ft3ds_on_903ds_2e_0.928_len64_gelu_checkpoint-1926\n",
    "onnx_model = '/kaggle/input/onnx-xs2-l64/xs2_len64_onnx/model.onnx'                            # based on: xs2_ft3ds_on_3dscheckpoint-29630_len64_3e_gelu_checkpoint-2889\n",
    "\n",
    "\n",
    "df = pd.read_json(data_filepath)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df, preserve_index=False)\n",
    "dataset = DatasetDict({'train': train_dataset})\n",
    "print(dataset)\n",
    "print(len(dataset['train']))\n",
    "\n",
    "if len(dataset['train']) < 1000:\n",
    "    data_filepath = '/kaggle/input/pii-detection-removal-from-educational-data/train.json'\n",
    "    print('It is not a submission! Overriding test dataset to train.json!')\n",
    "    df = pd.read_json(data_filepath)\n",
    "    train_dataset = Dataset.from_pandas(df, preserve_index=False)\n",
    "    dataset = DatasetDict({'train': train_dataset})    \n",
    "    print(dataset)\n",
    "    print(len(dataset['train']))\n",
    "\n",
    "original_dataset = dataset\n",
    "original_df = df\n",
    "\n",
    "document_to_rows = {}\n",
    "for i, row in tqdm(original_df.iterrows()):\n",
    "    document_to_rows[row['document']] = row\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------- add id_num directly to csv based on simple rule\n",
    "start = time.time()\n",
    "csv_simple_rule_rows = []\n",
    "\n",
    "# print('normalizing...')\n",
    "# #df['tokens'] = df['tokens'].str.normalize('NFKD') \n",
    "# df['tokens'] = df['tokens'].apply(lambda tokens: [unicodedata.normalize('NFKD', token) for token in tokens])\n",
    "# print('normalized')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    names = []\n",
    "    namesi = []\n",
    "    for i,token in enumerate(row.tokens):\n",
    "        if len(token[-6:]) >= 6 and token[-6:].isnumeric() and 'http' not in token:\n",
    "            names.append(token)\n",
    "            namesi.append(i)\n",
    "    \n",
    "    for i, token in zip(namesi, names):\n",
    "        if 'x' in token and token.replace('x','').isdigit() or ')' in token or '.' in token and token.replace('.','').isdigit():\n",
    "            print('PHONE_NUM: ', i, token)\n",
    "            fake_label = 'I-PHONE_NUM'\n",
    "        elif len(token) == 5 and token.isdigit():\n",
    "            print('ADDRESS: ', i, token)\n",
    "            fake_label = 'I-STREET_ADDRESS'\n",
    "        else:\n",
    "            fake_label = 'B-ID_NUM'\n",
    "            print('ID_NUM: ', i, token)\n",
    "        csv_simple_rule_rows.append([0, row.document, i, fake_label])\n",
    "\n",
    "print('csv_simple_rule_rows for ID_NUM populated: ', len(csv_simple_rule_rows))\n",
    "\n",
    "# sys.exit()\n",
    "stop = time.time()\n",
    "print(csv_simple_rule_rows)\n",
    "print('csv_simple_rule_rows for ID_NUM time: ', time.time() - start)\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "print('building all_lower_set...')\n",
    "all_lower = []\n",
    "for index, row in df.iterrows():\n",
    "    lower_tokens = [token for token in row.tokens if token.islower()]\n",
    "    all_lower.extend(lower_tokens)\n",
    "all_lower_set = set(all_lower)\n",
    "\n",
    "\n",
    "\n",
    "local_files_only_flag = False\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(base_model, local_files_only=local_files_only_flag, additional_special_tokens=['[SP]', '[NL]'])\n",
    "tokenizer20 = AutoTokenizer.from_pretrained(inference_model_name2, local_files_only=local_files_only_flag, additional_special_tokens=['[SP]', '[NL]'])\n",
    "\n",
    "tokenizer = tokenizer1\n",
    "\n",
    "\n",
    "labels = ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O']\n",
    "labels = labels[-1:] + labels[:-1] #for convenience, so O will be at index 0\n",
    "\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "for i, label in enumerate(labels):\n",
    "    id2label[i] = label\n",
    "    label2id[label] = i\n",
    "\n",
    "\n",
    "tokenizer_kwargs = {'max_length': 64, 'padding': 'max_length', 'truncation': True, 'return_overflowing_tokens': True, 'stride':12 }\n",
    "\n",
    "def tokenize_function(example):\n",
    "    start = time.time()\n",
    "    tokens_batch = example['tokens']\n",
    "    tokens_batch = [[t.replace('\\n', '[NL]').replace(' ', '[SP]').replace('\\u200b', '') for t in tokens] for tokens in tokens_batch] #TODO it assumes batches only\n",
    "    result = tokenizer(tokens_batch, **tokenizer_kwargs, return_tensors='np', is_split_into_words=True) #np vs pt\n",
    "    stop = time.time()\n",
    "    print('tokenize_function time: ', stop - start)\n",
    "    return result\n",
    "\n",
    "def tokenize_and_align_original_columns_new(examples):\n",
    "    tokenized_inputs = tokenize_function(examples)\n",
    "\n",
    "    word_ids_batch = []\n",
    "    tokens = []\n",
    "    full_text = []\n",
    "    document = []\n",
    "    \n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    overflow_to_sample_mapping = []\n",
    "    tokens_list = examples['tokens']\n",
    "#     np.object = object #problem was in lack of np.asarray\n",
    "    for i, original_i in enumerate(tokenized_inputs['overflow_to_sample_mapping']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "#         word_ids_batch.append(word_ids)\n",
    "        \n",
    "        word_ids_non_none = [id for id in word_ids if id is not None]\n",
    "        first_word_id = word_ids_non_none[0]\n",
    "        last_word_id = word_ids_non_none[-1]\n",
    "        \n",
    "        tt = tokens_list[original_i][first_word_id:last_word_id+1]\n",
    "        if not check_for_pii(tt):\n",
    "            continue\n",
    "        \n",
    "        word_ids_batch.append(word_ids)\n",
    "        tokens.append(examples['tokens'][original_i][first_word_id:last_word_id+1]) #that first/last wasnt here before\n",
    "        full_text.append(examples['full_text'][original_i])\n",
    "        document.append(examples['document'][original_i])\n",
    "        \n",
    "        input_ids_i = tokenized_inputs.input_ids[i]\n",
    "        input_ids.append(input_ids_i)\n",
    "        token_type_ids_i = tokenized_inputs.token_type_ids[i]\n",
    "        token_type_ids.append(token_type_ids_i)\n",
    "        attention_mask_i = tokenized_inputs.attention_mask[i]\n",
    "        attention_mask.append(attention_mask_i)\n",
    "        overflow_to_sample_mapping_i = tokenized_inputs.overflow_to_sample_mapping[i]\n",
    "        overflow_to_sample_mapping.append(overflow_to_sample_mapping_i)\n",
    "\n",
    "    tokenized_inputs['word_ids'] = word_ids_batch\n",
    "    tokenized_inputs['tokens'] = tokens\n",
    "    tokenized_inputs['full_text'] = full_text\n",
    "    tokenized_inputs['document'] = document\n",
    "    \n",
    "    tokenized_inputs['input_ids'] = np.asarray(input_ids)\n",
    "    tokenized_inputs['token_type_ids'] = np.asarray(token_type_ids)\n",
    "    tokenized_inputs['attention_mask'] = np.asarray(attention_mask)\n",
    "    tokenized_inputs['overflow_to_sample_mapping'] = np.asarray(overflow_to_sample_mapping)\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "example = dataset['train'][0]\n",
    "tokenized_example = tokenize_function(example) #fire once so caching works\n",
    "\n",
    "columns_to_remove = ['trailing_whitespace', 'full_text', 'document', 'tokens']\n",
    "if 'labels' in dataset['train'][0]:\n",
    "    columns_to_remove.append('labels')\n",
    "\n",
    "print('columns_to_remove: ', columns_to_remove)\n",
    "\n",
    "tokenizer = tokenizer1\n",
    "dataset20 = dataset\n",
    "\n",
    "\n",
    "print('dataset:')\n",
    "print(dataset)\n",
    "print('dataset20:')\n",
    "print(dataset20)\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "if NAMES_SELECTION == 1:\n",
    "    all_names_file = '/kaggle/input/pii-real-names-dataset/given_names_data.parquet'\n",
    "    names7 = pd.read_parquet(all_names_file, columns=['given_name'])\n",
    "    names7 = names7['given_name']\n",
    "    names7 = [name for name in names7 if name != None]\n",
    "    names7 = set(names7)\n",
    "    all_names = names7\n",
    "    print('all_names contains: ', len(all_names), 'names')\n",
    "elif NAMES_SELECTION == 2:\n",
    "    #N7\n",
    "    all_names_file = '/kaggle/input/all-namesn7c/all_namesN7C.pkl'\n",
    "    with open(all_names_file, 'rb') as f: \n",
    "        namesN7C = pickle.load(f)\n",
    "    all_names = namesN7C\n",
    "\n",
    "    #N7\n",
    "    all_names_file = '/kaggle/input/all-namesw/all_namesW.pkl'\n",
    "    with open(all_names_file, 'rb') as f: \n",
    "        namesW = pickle.load(f)\n",
    "    all_names |= namesW\n",
    "\n",
    "    print('all_names contains: ', len(all_names), 'names')\n",
    "else:\n",
    "    all_names = None\n",
    "    print('proceeding without names lists')\n",
    "\n",
    "stop = time.time()\n",
    "print('all_names load time: ', stop - start)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "stop_words1 = ['We', 'They', 'This', 'Our', 'Out', 'The', 'To', 'How', 'He', 'Then', 'By', 'Also', 'Here', 'Mind', 'Story', 'Tool', 'You', 'In', 'When', 'If', 'Not', 'Some', 'My', \n",
    "              'After', 'Since', 'Why', 'As', 'Did', 'One', 'So', 'There', 'Once', 'Before', 'But']\n",
    "stop_words2 = ['A', 'At', 'All', 'From', 'Next', 'On', 'First', 'Innovation', 'Example', 'Most', 'India', 'Visual', 'Many', 'Thus', 'Based', 'Assignment', 'She', 'Hence', 'Even', 'Map', 'An', 'Every', 'Given', 'Is', 'Data', \n",
    "'Sales', 'Due', 'Being', 'Use', 'Journey', 'English', 'No', 'New', 'Future', 'Well', 'Value', 'Second', 'More', 'Do', 'Moving', 'According', 'Final', 'Human', 'Good', 'Firstly', 'Post', 'Like', 'Key', 'Other',\n",
    " 'Step', 'Idea', 'Just', 'School', 'Time', 'Overall', 'Indian', 'Let', 'Chain', 'App', 'Finance', 'Both', 'Maps', 'Make', 'Later', 'Bank', 'Despite', 'Page', 'Agile', 'Can', 'Name', 'Going', 'Peer', 'Last', \n",
    " 'Brand', 'Ideas', 'Brazil', 'Over', 'Quality', 'Student', 'Tools', 'General', 'South', 'Director', 'Her', 'Life', 'Are', 'Me', 'Congress', 'Be', 'Africa', 'Rather', 'Or', 'Analysis', 'Knowledge', 'Senior',\n",
    " 'Cost', 'Power', 'Along', 'Thank', 'Maybe', 'Growth', 'Head', 'State', 'Plan', 'Works', 'Lack', 'Define', 'Europe', 'Any', 'Low', 'Among', 'Stage', 'Regarding', 'Non', 'Work', 'Seeing', 'Phase', 'Mexico', 'Big', 'Have', 'Start', 'America', 'Kitchen', 'Apart',\n",
    " 'Take', 'Day', 'African', 'Co', 'Part', 'Focus', 'Risk', 'Children', 'Does', 'Yet', 'Transformation', 'Right', 'Smart', 'Parents', 'Date', 'Level', 'Lead', 'Excel', 'Core', 'Media',\n",
    " 'Case', 'Current', 'Drawing', 'Few', 'Care', 'Peru', 'Point', 'Cloud', 'Zoom', 'States', 'Teachers', 'Better', 'March', 'Note', 'End', 'Master', 'Delivery', 'Develop', 'Main', 'China', 'Web',\n",
    " 'Very', 'Chinese',  'Prior', 'Apple', 'Real', 'Self', 'Platform', 'East', 'Small', 'Impact', 'September', 'Get', 'Elements', 'Spanish', 'Singapore', 'Vision', 'Canvas',\n",
    " 'Wow', 'Ask', 'Back', 'October', 'Simple', 'Industrial', 'Australia', 'December', 'Add', 'Lean', 'January', 'Central', 'Go',\n",
    " 'What', 'People', 'Company', 'Project', 'Internet', 'Video', 'Your', 'Two', 'Experience', 'Only', 'High',  'United', 'City', 'Third', 'Science', 'American', 'Together', 'Keeping', 'Professor', 'World', 'Three',\n",
    " 'Mr',\n",
    "#  'Apt',\n",
    " ]\n",
    " #'Mr',\n",
    "    \n",
    "stop_words3 = [ 'Pakistan',  'Red', 'Intelligence', 'Nigeria', 'Task', 'Keep', 'Increase', 'Staff', 'Had', 'Soon', 'June', 'Latino', 'Amazon', \n",
    "    'November', 'Peace', 'Imagine', 'Centre', 'North', 'Live', 'Home', 'French', 'Need', 'Persona', 'See', 'York', 'Put', 'Ans', 'Cash', 'Flow', 'Pre', 'Best', 'Partner', 'Price', 'Author', \n",
    "    'Was', 'Fast', 'Job', 'Still', 'Farmers', 'Content', 'Study', 'Argentina', 'Play', 'Lego', 'Find', 'Open', 'Field', 'Spain', 'Share', 'July', 'German', 'San', 'Little', 'Oil', 'Free', \n",
    "    'August', 'Great', 'Leaders', 'Target', 'Loan', 'Similar', 'Minimum', 'Should', 'Graphic', 'Russia', 'Germany', 'Same', 'Various', 'Coffee', 'Pilot', 'Talent', 'Give', 'Selected', \n",
    "    'Shop', 'Hiring', 'Club', 'Middle', 'Lab', 'Voice', 'Around', 'Culture', 'Act', 'Game', 'Plus', 'Friday', 'Up', 'Gas', 'Brands', 'Title', 'Jeanne', 'Top', 'Brain', 'Area', 'Major', \n",
    "    'Wi', 'Fi', 'Presentation', 'Air', 'Word', 'Six', 'Draw', 'Banks', 'France', 'Method', 'Physical', 'List', 'Korea', 'Image', 'Energy', 'Campus', 'Scientific', 'British', 'Beyond', 'Long', \n",
    "    'Show', 'None', 'Fig', 'Root', 'Net', 'Goal', 'Monday', 'Present', 'Steps', 'Try', 'Set', 'Faculty', 'Age', 'Select', 'Majority', 'Russian', 'Medium', 'Material', 'Asking', \n",
    "    'Moscow', 'Patients', 'Tesla', 'Straw', 'Much', 'Allow', 'Made', 'Greek', 'Success', 'February', 'Pin', 'Sector', 'Christmas', 'Tell', 'Water', 'Line', 'Selling', 'Traditional', \n",
    "    'Used', 'Person', 'Reason', 'Cross', 'Urban', 'Less', 'Rapid', 'Beauty', 'Choice', 'Excellence', 'Four', 'Discovery', 'Innovative', 'Table', 'Canada', 'Coaching', 'Choose',\n",
    "    'Agents', 'Local', 'Too', 'Saturday', 'Lagos', 'Role', 'Mental', 'Kenya', 'Portal', 'Law', 'Lunch', 'Higher', 'Matter', 'Critical', 'Short', 'Videos', 'Ecuador', 'Accounting', \n",
    "    'Jobs', 'Suddenly', 'Car', 'Call', 'Re', 'Cyber', 'Pension', 'Vice', 'Chile', 'Avoid', 'Asset', 'Large', 'Force', 'Mural', 'Standard', 'Positive', 'Child', 'Card', \n",
    "    'Honestly', 'Consider', 'Blue', 'Rimini', 'Ghana', 'Zone', 'Read', 'Multi', 'Room', 'Avion', 'Bachelor', 'Firm', 'Trust', 'Fortune', 'Channel', 'God', \n",
    "    'Transport', 'Sale', 'Index', 'Purpose', 'Solar', 'Kids', 'Interview', 'Ref', 'Paris', 'Adobe', 'Score', 'Half', 'Western', 'Book', 'Offer', 'Changes', 'Primary', 'Eastern',\n",
    "    'Coach', 'Ex', 'Speed', 'Doctors', 'Region', 'Grade', 'Capital', 'Stress', 'Black', 'Delhi', 'Front', 'Greece', 'Face', 'Tree', 'Radio', 'Hong', 'Kong', 'Serious', 'Topic', \n",
    "    'Stanford', 'Discover', 'Laundry', 'Early', 'Active', 'Sample', 'Till', 'London', 'Alpha', 'Pitch', 'Look', 'Carbon', 'Gender', 'Return', 'Uber', 'Probably', 'Member', 'Year', 'Divine', \n",
    "    'Manual', 'Teacher', 'Uruguay', 'Previous', 'Drive', 'Press', 'Bus', 'Kingdom', 'Switzerland', 'Airport', 'Answer', 'Clear', 'Patient', 'Balance', 'Vehicle', 'True', 'Engine', 'Sr', 'Maersk', \n",
    "    'Native', 'Hotel', 'Challange', 'Everyday', 'Poor', 'Situation', 'Fail', 'Buyer', 'Device', 'Principal', 'Nature', 'Modern', 'Desing', 'Natural', 'Done', 'Quito', 'Logo', \n",
    "    'Slowly', 'Win', 'Tuesday', 'Park', 'Einstein', 'Basic', 'Pay', 'Buy', 'Movie', 'Buddy', 'Hi', 'Lecture', 'Were', 'Quite', 'Past', 'Deep', 'Break', 'Color', 'Breaking', 'Anyway', \n",
    "    'Lots', 'Gain', 'Quick', 'Math', 'Dev', 'Disney', 'Masters', 'Town', 'Remote', 'Juniors', 'Bengali', 'Economy', 'Property', 'Record', 'Perfect', 'Normal', 'Coca', 'Cola', \n",
    "    'Parks', 'Cluster', 'Abu', 'Location', 'View', 'Provider', 'Mid', 'Unique', 'Than', 'Degree', 'Chemical', 'Anti', 'Happy', 'Anxiety', 'Traffic', \n",
    "    'Ways', 'Packaging', 'Possible', 'Follow', 'Release', 'Might', 'Commerce', 'Aristotle', 'Circle', 'Admin', 'Mumbai', 'Verso', 'Bitcoin', 'Maintain', 'Parts', 'Profit', 'Has', \n",
    "    'Journal', 'Guide', 'Off', 'La', 'Stream', 'Latinos', 'Tax', 'Agreement', 'Moderate', 'Full', 'Proof', 'Box', 'Kit', 'Learners', 'Northern', 'Mission', 'Gen', 'Uganda', 'Etc', \n",
    "    'Leader', 'Peruvian', 'Double', 'House', 'Indians', 'Bora', 'Chapter', 'Wednesday', 'Sri', 'Jira', 'Pressure', 'Iran', 'Drug', 'Micro', 'Eg', 'Zero', 'Meet', 'Pueblo', 'Demo', \n",
    "    'São', 'Pi', 'Loss', 'Practical', 'Decide', 'Motor', 'Stock', 'Rio', 'Renaissance', 'Link', 'Founder', 'Lake', 'Roll', 'Owner', 'Train', 'Arabia', 'Volume', 'Arab', 'Log', \n",
    "    'Strong', 'Madrid', 'Artists', 'Hub', 'Azerbaijan', 'Stay', 'Feb', 'Station', 'Say', 'Ten', 'Arts', 'Jordan', 'Brush', 'Potential', 'Integrated', 'Print', 'Yard', 'Aires', 'Rome', \n",
    "    'Either', 'Fall', 'Triple', 'Kia', 'Pain', 'Round', 'Mood', 'Hand', 'Stick', 'Base', 'Daily', 'Proper', 'Lesson', 'Pillar', 'Hat', 'Tea', 'Lanka', 'Fourth', 'Bit', 'Oral', 'Capture', 'Dutch', \n",
    "    'Got', 'Complete', 'Hire', 'Bring', 'Burundi', 'Civil', 'Tank', 'Never', 'Detail', 'Assurance', 'Court', 'Persian', 'Mens', 'Amsterdam', 'Battery', 'Running', \n",
    "    'Confidence', 'Instructor', 'Pacific', 'Areas', 'Trial', 'Advisory', 'Pick', 'Climate', 'Code', 'Progress', 'Garden', 'Men', 'Way', 'Political', 'Happiness', 'Sony', 'Millennial', 'Medicine',\n",
    "    'Puerto', 'Listen', 'Alternate', 'Fixed', 'Buenos', 'Suite', 'Inspiring', 'Berlin', 'Shoe', 'Excellent', 'Size', 'Mall', 'Okay', 'Direction', 'Begin', 'Printing',\n",
    "    'Oh', 'Regular', 'Sea', 'Summit', 'Ops', 'Loans', 'Sponsor', 'Vinci', 'Option', 'Beta', 'Living', 'Text', 'Forward', 'Sun', 'Frame', 'Gap', 'Namibia', 'Wall', \n",
    "    'Debit', 'Chaplin', 'Am', 'Skill', 'Hood', 'Collect', 'Taiwan', 'Item', 'Bay', 'Brussels', 'Barcelona', 'Bike', 'Love', 'Mirror', 'Tourism', 'Pride', 'Trainer', \n",
    "    'Days', 'Ishikawa', 'Vietnam', 'Bid', 'Texas', 'Makes', 'Potter', 'Reverse', 'Sketch', 'Golden', 'Nov', 'Baby', 'Feel', 'Rs', 'Min', 'Nigerian', 'Investors', \n",
    "    'Port', 'Ability', 'Hard', 'Surely', 'Manufacturer', 'Blind', 'Light', 'Display', 'Bar', 'Quickly', 'Knapp', 'Citizen', 'Chronic', 'Lets', 'Lines', \n",
    "    'Come', 'Bible', 'Pattern', 'Deliver', 'Bloom', 'Close', 'Successful', 'Trash', 'Theme', 'Yoga', 'Telco', 'Diamond', 'Seek', 'Dairy', 'Lawn', 'Window', 'Myanmar', 'Essential', 'Kind',\n",
    "    'Continue', 'Reaction', 'Deutsche', 'Cart', 'War', 'Century', 'Maths', 'Regards', 'Ok', 'Enter', 'Bello', 'Ancient', \n",
    "    'Wallet', 'Cooking', 'Shift', 'Goods', 'Grow', 'Thursday', 'Java', 'Los', 'Times', 'Highly', 'Audit', 'Clarity', 'Mode', 'Ever', 'Toulouse', 'Sudan', 'Leave', 'Contracting', \n",
    "    'Munich', 'Late', 'Parking', 'Able', 'Save', 'Body', 'Constant', 'Welcome',  'Ford', 'Bi', 'Mobility', 'Scientist', 'Grand', 'Levels', 'Server', 'Easter', 'Cafe',\n",
    "    'Guatemala', 'Ifs', 'Wish', 'Road', 'River', 'Linear', 'Bot', 'Pie', 'Empathise', 'Temp', 'Fun', 'Don', 'Faster', 'Quiz', 'Significant', 'Vary', 'Leisure', \n",
    "    'Via', 'Connect', 'Bengal', 'Flip', 'Smartphone', 'Flexible', 'Milan', 'Finca', 'Forest', 'Upcoming', 'Bundle', 'Believe', 'Valley', 'Guest', 'Kaiser', 'Millions',\n",
    "    'Coke', 'Canal', 'Benz', 'Importantly', 'Enough', 'Ideal', 'Mol', 'Semi', 'Vs', 'Telekom', 'Speaker', 'Parent', 'Hero', 'Trip', 'Journeys', 'Mac', \n",
    "    'Rotaract', 'Prefect',  'Racing', 'Jefferson', 'Soldiers', 'Charity', 'Artist', 'Rooms', 'Huge', 'Dec', 'Nairobi', 'Africans', 'Respect', 'Janeiro', 'Wise', 'Mein', \n",
    "    'Olympic', 'Moral', 'Lecturer', 'Missing', 'Hot', 'Colour', 'Counselor', 'Doc', 'Sure', 'Them', 'Affordable', 'Mentor']\n",
    "    #'Mars', 'Myer', 'Will', 'Meyer', 'Geoff', 'Miro', 'April', 'Ed', 'Andre', 'Tim', 'Alice', 'Hesse', 'Marian', 'Madeleine', 'Venn', 'Petty', 'Sarkar', 'Rob', 'Jane', 'Rya', 'Hellen', 'Norman', \n",
    "    #'Georgia', 'Hess', 'Paulo', 'Mark', 'Dan', 'Frank', 'Prezi', 'Ravi', 'Tom', 'Moses', 'Joana', 'Bob', 'Vicente', 'Eric', 'Victor', 'Jim', 'Louis', 'Tommy', 'Stefan', 'Rik', 'Bharti', 'Albert', \n",
    "    #'Kearney', 'Barry', 'Mercedes', 'Johari', 'Rose', 'Kerala', 'Sindhi', 'Davis', 'Lotte', 'Alexander', 'Henry', 'Dean', 'Hill', 'Mayer', 'Ramon', 'Tienda', 'Pago', 'Indus', \n",
    "    #'Charlie', 'Muller', 'Maori', 'Medellin', 'Marias', 'Sinek', 'Bono', 'Pragati', 'Mason', 'Edward', 'Luke', 'Churro', 'Malak', 'Wema', 'Hall', 'Sacco', \n",
    "    #'Anthony',\n",
    "    #'Kelley', 'Keller', 'Rica', 'Pune', 'Monterrey', 'Tata', 'Tapas', 'Summer', 'Carr', \n",
    "\n",
    "stop_words9 = ['For', 'Design', 'And', 'With', 'While', 'During', 'Now', 'Business', 'Team', 'Telling', 'Week', 'Process', 'Working', 'Service', 'Health', 'Group', 'Such', 'Board', \n",
    "'Change', 'Office', 'Technical', 'Client', 'Market', 'Course', 'Yes', 'Model',  'Test', 'Getting', 'Mobile',  'Doing',  'Center', 'Building', \n",
    " 'College', 'National',  'Public',  'Below', 'Members', 'Learn','Store', 'Latin', 'Engineer',  'Chief',\n",
    " 'Number',  'President', 'Problems', 'Section', 'Legal', 'Easy', 'Meeting', 'Check', 'Mexican', 'Women', 'Paper', 'Officer', \n",
    " 'Credit', 'Career', 'Cases', 'Session', 'Young', 'Tech', 'Chart', 'Subject', 'Private', 'Write', 'Startup',  'Scope', 'Action',\n",
    " \n",
    " 'Korean', 'Indonesia', 'Machine', 'Execution', 'Policy', 'Money', 'Control', 'Above', 'Regional', 'Egypt', 'Special', 'Benefits', 'Café', 'Japanese', 'Council', 'Confidential',\n",
    "'Japan', 'Reading', 'Turkey', 'Cause', 'White', 'Payment', 'Survey', 'Plant', 'Built', 'Know', 'Family', 'Operation', 'Hearing',  'Setting', 'Helps', 'Class', 'Schools',  'Clean', \n",
    "'Soldier', 'Columbia', 'Electric', 'Demand', 'Needs', 'Matrix', 'Access', 'Common', 'Search', 'Mother', 'Estate', 'Place', 'Five', 'Branch', 'Fashion', 'Space', 'Words', 'Talk', 'Trade',\n",
    "'Nations', 'Scale', 'Certain', 'Walk', 'Theory', 'Labor', 'Electronic', 'Union', 'Payroll', 'Federal', 'Metro', 'Lower', 'Sports', 'Fuel', 'Magazine', 'Apps', 'Form', 'Calendar', 'Patent',\n",
    "'Billing', 'Rate', 'Drilling', 'Rule', 'Points', 'Years', 'Contact', 'Secret', 'Chicago', 'Audio', 'Country', 'Music', 'Oracle', 'Rural', 'Italy', 'Supply', 'Asian', 'Finding',\n",
    "'Reality', 'Travel', 'Simply', 'Waste', 'Gallery', 'Diary', 'Saudi', 'Brief', 'Father', 'Learner', 'Egyptian', \n",
    "'C', 'T', 'N', 'M', 'L',\n",
    "'Roam', 'Bad', 'Windows', 'Purchase', 'Agent', 'Riding', 'Games', 'Unknown', 'Earth', 'Maximum', 'Tables',  'Old', 'Track', 'Orders', 'Army',\n",
    "'Americas', 'Crazy', 'Player', 'Loyalty', 'Heads', 'Carrier', 'Palestine', 'Channels', 'Cancer', 'Auto', 'Chat', 'Stake', 'Contractor',  'Centers', 'Single', 'Venezuela', 'Navy',  'Guys',\n",
    " 'Neither', 'Pricing', 'Trello', 'Domain', 'Stores', 'Cheap', 'Southern', 'Friends', 'Boards', 'Born', 'Canadian', 'Screen', 'Worth', 'Forum', 'Animal', 'Meal', 'Conflict',\n",
    " 'Thailand', 'Sitting', 'Secretary', 'Blood', 'Character', 'Affinity', 'Female', 'Essay', 'Platinum', 'Turkish', 'Robot', 'Taken', 'Logic', 'Wife', 'Buyers', 'Click', 'Saving',\n",
    " 'Weeks', 'Farmer', 'Weight', 'Ready', 'Distance', 'Sweden', 'Cup', 'Guardian', 'Sport', 'Turn', 'Freedom', 'Zealand', 'Sub',\n",
    " #2:\n",
    " 'I', 'Insight', 'Learning', 'Finally', 'Development', 'Digital', 'B', 'Where', 'Its', 'Create', 'Making', 'His',\n",
    " #3:\n",
    "#'Rehab', 'Dip', \n",
    "'Junior', 'West', 'Moore', 'Pink', 'Baker', 'Green', 'Gray', 'Brown',\n",
    "# 'St.', \n",
    " ]\n",
    "\n",
    "stop_wordsU = ['Brazil', 'Mexico', 'Peru', 'September', 'Singapore', 'Australia', 'January', 'Pakistan', 'Nigeria', 'Korean', 'Indonesia', 'Latino', 'Italy', \n",
    "'Spain', 'Russia', 'Costa', 'France', 'Korea', 'British', 'Russian', 'Moscow', 'Thailand', 'Canada', 'Saturday', 'Lagos', 'Kenya',\n",
    "'Chile', 'Ghana', 'Columbia', 'Delhi', 'Brazilian',\n",
    "'Greece', 'London', 'Uber', 'Tuesday', 'Arabic', 'Mumbai', 'Latinos', 'Uganda', 'Arabia', 'Madrid', 'Feb', 'Aires', 'Rome', 'Dutch', \n",
    "'Americas', 'Persian', 'Mens', 'Amsterdam', 'Emirates', 'Sony', 'Berlin', 'Barcelona', 'Vietnam', 'Texas', 'Nov', 'Nigerian', 'Rican', 'Canadian', 'Harvard', 'Myanmar']\n",
    "    \n",
    "stop_words9M = ['Accepted', 'Advance', 'Aid', 'Aided', 'Aim', 'Algebra', 'Alliance', 'Aloe', 'Amethyst', 'Anchors', 'Arduino', 'Autumn', \n",
    "'Axis', 'Badge', 'Banana', 'Batch', 'Beach', 'Bear', 'Bench', 'Beverage', 'Bias', 'Bidding', 'Bio', 'Birth', 'Blanks', 'Block', 'Bold', 'Books', 'Booth', \n",
    "'Borders', 'Boss', 'Boy', 'Boys', 'Broaden', 'Broker', 'Broking', 'Bugs', 'Bulk', 'Bureau', 'Burns', 'Butterfly', 'Calm', 'Camera', 'Capability', 'Careers', \n",
    "'Carnival', 'Causing', 'Cell', 'Chairs', 'Champion', 'Charter', 'Chicken', 'Chosen', 'Church', 'Cinderella', 'Classified', 'Coast', 'Cold', 'Compact', 'Compass', \n",
    "'Convey', 'Corners', 'Corning', 'Corona', 'Count', 'Counting', 'Counts', 'Courage', 'Courier', 'Covert', 'Craft', 'Crisis', 'Crowd', 'Crystal', 'Cut', 'Dad', \n",
    "'Damper', 'Dear', 'Defer', 'Delay', 'Deputy', 'Dollar', 'Draft', 'Dream', 'Drew', 'Drivers',\n",
    "'Drone', 'Dry', 'Dual', 'Dummy', 'Dust', 'Duty', 'Elite', 'Epic', 'Error', 'Evangelist', 'Evening', 'Eye', 'Factor', 'Failing', 'Farm', 'Fear', 'Fears', 'Fest', 'Fidelity', 'Figures',\n",
    "'Fill', 'Filling', 'Film', 'Fine', 'Fire', 'Fiscal', 'Fish', 'Fit', 'Fitting', 'Fix', 'Flag', 'Flash', 'Flourish', 'Football', 'Former', 'Fort', 'Forty', 'Foster', \n",
    "'Fresh', 'Friend', 'Friendly', 'Fries', 'Funny', 'Galaxy', 'Gather', 'Gave', 'Gaze', 'Gentle', 'Geo', 'Giant', 'Gift', 'Gig', 'Girl', 'Glass', 'Globe', 'Gold', 'Gov', \n",
    "'Grab', 'Grammar', 'Greenhouse', 'Greet', 'Grip', 'Guard', 'Guess', 'Guru', 'Hall', 'Hands', 'Hardware', 'Heading', 'Healing', 'Heavy', 'Hey', 'Hires', 'Historical', 'Hobby', \n",
    "'Honesty', 'Hundred', 'Hunger', 'Hunting', 'Ice', 'Inception', 'Ink', 'Island', 'Jaguar', 'Jealousy', 'Joint', 'Judge', 'Jug', 'Justice', 'Kick', 'Kill', 'Kinda', \n",
    "'Label', 'Labour', 'Labs', 'Lamp', 'Land', 'Lately', 'Laws', 'Layer', 'Leather', 'Leaving', 'Led', 'Ledger', 'Left', 'Legend', 'Letters', 'Links', 'Losing', 'Lot', 'Lovers', \n",
    "'Macro', 'Magic', 'Mail', 'Mailer', 'Maker', 'Manger', 'Marking', 'Mask', 'Mass', 'Match', 'Matters', 'Mature', 'Meals', 'Meats', 'Memory', 'Memos', 'Mention', 'Messenger', \n",
    "'Meta', 'Meter', 'Middleman', 'Mile', 'Milestone', 'Milk', 'Mins', 'Modes', 'Mom', 'Moment', 'Monster', 'Mountain', 'Muslim', 'Must', 'Mystery', 'Mythology', 'Nail', 'Names',\n",
    "'Necessary', 'Needle', 'News', 'Noble', 'Numbers', 'Older', 'Ones', 'Orange', 'Organized', 'Original', 'Orphan', 'Ours', 'Pages', 'Paint', 'Painting', 'Papers',  \n",
    "'Party', 'Pass', 'Passion', 'Path', 'Pause', 'Paying', 'Peak', 'Pen', 'Penning', 'Persons', 'Pet', 'Piano', 'Pic', 'Picking', 'Piece', 'Pillars', 'Planes', 'Plants', \n",
    "'Polish', 'Poll', 'Pop', 'Ports', 'Poster', 'Pretty', 'Prime', 'Pro', 'Proto', 'Province', 'Queen', 'Query', 'Random', 'Raw', 'Rear', 'Reasons', 'Renew', 'Reps', 'Rest', \n",
    "'Reveal', 'Rift', 'Rise', 'Rising', 'Roads', 'Roles', 'Roof', 'Rope', 'Run', 'Rustic', 'Samples', 'Sat', 'Schooling', 'Seat', 'Seed', 'Seeds', 'Seems', 'Sell', 'Seller', 'Sellers',\n",
    "'Selves', 'Send', 'Sense', 'Sensing', 'Sessions', 'Seven', 'Severe', 'Sexual', 'Shale', 'Shall', 'Sheets', 'Shorts', 'Signal', 'Silver', 'Sir', 'Sister', 'Slack', 'Sleep', \n",
    "'Slide', 'Smile', 'Soap', 'Solid', 'Spare', 'Speak', 'Spinning', 'Stack', 'Standing', 'Streets', 'Strength', 'Stuck', 'Style', 'Sunshine', 'Surprise', 'Sweets', \n",
    "'Tab', 'Tag', 'Tap', 'Taste', 'Teach', 'Tears', 'Teen', 'Threat', 'Throwing', 'Tier', 'Tittle', 'Tobacco', 'Topping', 'Touch', 'Tour', 'Toy', 'Tracker', 'Trader', 'Trading', \n",
    "'Transmission', 'Traveller', 'Trend', 'Truck', 'Truly', 'Trunk', 'Truth', 'Tuning', 'Turns', 'Twin', 'Upper', 'Us', 'Vegan', 'Venture', 'Vivid', 'Wait', 'Walking', 'Wanna', 'Want', \n",
    "'Warming', 'Warning', 'Wash', 'Wearing', 'Weekly', 'Wheat', 'Wheeler', 'Whistle', 'Wide', 'Wild', 'Wind', 'Wing', 'Winner', 'Winning', 'Wisdom', 'Worker', 'Worst', 'Yellow']\n",
    "#'Apt',\n",
    "#'Plenty', 'Paradise', 'Spark', 'Delta', 'Down', 'Sky', \n",
    "\n",
    "stop_wordsF = ['India', 'An', 'Can', 'Peer', 'Among', 'Lean', 'German', 'Loan', 'Made', 'Each']\n",
    "\n",
    "stop_words10 = ['Colombia', 'Philippines', 'Inc', 'Bangladesh', 'Universidad', 'Netherlands', 'Dubai', 'Aramco', 'Starbucks', 'Confluence', 'México', 'Bogotá', 'Netflix', \n",
    "'Alzheimer', 'Symbian', 'Capgemini', 'Americans', 'Paraguay', 'Pixar', 'Australian', 'Airbnb', 'Unicamp']\n",
    "\n",
    "\n",
    "#'No', 'David', 'Angela', 'Name', 'Africa', 'Virginia', 'Mars', 'May', 'John', 'Asia', 'Will', 'Ali', 'George', 'Ahmed', 'Martin', 'Art', 'Apple', 'Jose', \n",
    "#'Sunday', 'Princess', 'Steve', 'Geoff', 'April', 'Maria', 'June', 'Daniel', 'Ans', 'Ed', 'Anna', 'July', 'Andre', 'Tim', 'Tina', 'Ogilvie', 'Luis', 'Thomas', 'Kumar', \n",
    "#'Jeanne', 'Junior', 'Khan', 'Alice', 'Francisco', 'Lima', 'Hesse', 'Garcia', 'Lopez', 'Johnson', 'Marian', 'Carlos', 'Adam', 'Person', 'Jesus', 'Tony', 'Emily', 'Madeleine'\n",
    "#'Petty', 'Antonio', 'Martinez', 'Michael', 'Car', 'Sam', 'Re', 'Sarkar', 'Smith', 'Vice', 'Gonzalez', 'Money', 'Tesla', 'Straw', 'Mr', 'Street', \n",
    "\n",
    "stop_wordsN7 = ['The', 'We', 'This', 'In', 'It', 'Application', 'Challenge', 'Selection', 'Approach', 'Insight', 'As', 'What', 'Mind', 'Visualization', 'For', 'After', 'Learning', \n",
    "    'Storytelling', 'Design', 'So', 'Our', 'Launch', 'To', 'Mapping', 'They', 'Thinking', 'Reflection', 'My', 'And', 'When', 'By', 'Then', 'If', 'With', 'There', 'But', 'One', 'At', \n",
    "    'However', 'These', 'Since', 'Also', 'Tool', 'While', 'How', 'Once', 'That', 'Each', 'During', 'All', 'From', 'Using', 'Now', 'Next', 'On', 'First', 'Innovation', 'Describe', \n",
    "    'Insights', 'Through', 'Therefore', 'Some', 'Business', 'Because', 'Example', 'He', 'Finally', 'Story', 'You', 'Most', 'India', 'Visual', 'People', 'Company', 'Many', 'Although', \n",
    "    'Management', 'Thus', 'Based', 'Hence', 'Assignment', 'She', 'Having', 'Even', 'Another', 'Customer', 'Map', 'Project', 'An', 'Before', 'Visualisation', 'Here', 'Every', 'Team', #An\n",
    "    'Additionally', 'Given', 'Is', 'Data', 'University', 'Why', 'Marketing', 'Product', 'Development', 'Sometimes', 'Due', 'Moreover', 'Instead', 'Not', 'Being', 'Manager', \n",
    "    'Stories', 'Use', 'Journey', 'Telling', 'English', 'Furthermore', 'No', 'Which', 'Week', 'Process', 'New', 'Working', 'Future', 'Everyone', 'Module', 'Well', 'Value', 'Students', #No\n",
    "    'Digital', 'More', 'Internet', 'Second', 'Do', 'Moving', 'Their', 'Department', 'User', 'According', 'Service', 'Human', 'Engineering', 'Final', 'Good', 'Of', 'Customers', 'Firstly', \n",
    "    'Those', 'Thanks', 'Google', 'Technology', 'Post', 'Key', 'Initially', 'Challenges', 'Secondly', 'Like', 'Facebook', 'Though', 'Often', 'Other', 'Scrum', 'Besides', 'Visio', 'Step', \n",
    "    'Just', 'School', 'Idea', 'Who', 'Research', 'System', 'Where', 'Time', 'Further', 'Problem', 'Applying', 'Overall', 'Currently', 'Indian', 'Recently', 'Within', 'Partners', 'Let', \n",
    "    'Health', 'Managers', 'Chain', 'Creating', 'App', 'Without', 'Finance', 'Following', 'Information', 'Unfortunately', 'Both', 'Maps', 'Make', 'Group', 'Later', 'Such', 'Coursera', \n",
    "    'Considering', 'Video', 'Your', 'Bank', 'Providers', 'Despite', 'Social', 'Board', 'Page', 'Its', 'Education', 'Agile', 'Instagram', 'Looking', 'Two', 'Reflection-', 'Create',  #Page\n",
    "    'Brainstorming', 'Covid-19', 'Can', 'Going', 'Name', 'Experience', 'Last', 'Peer', 'Designing', 'Taking', 'Organizations', 'Brand', 'Ideas', 'Understanding', 'Broadly', 'Brazil', \n",
    "    'Today', 'Over', 'Connected', 'Training', 'Change', 'Internal', 'Feedback', 'Different', 'Quality', 'Participants', 'Communication', 'Tools', 'Organization', 'Only', 'General', \n",
    "    'Especially', 'Office', 'Making', 'Director', 'South', 'Upon', 'Technical', 'Basically', 'Her', 'Client', 'Usually', 'Are', 'Market', 'Course', 'Yes', 'Operations', \n",
    "    'Planning', 'Starting', 'High', 'Leadership', 'Me', 'Life', 'Employees', 'Model', 'Oversight', 'Congress', 'Be', 'Rather', 'His', 'Africa', 'Software', 'Or', 'Listening', 'Program',  #Africa\n",
    "    'Empathy', 'Identify', 'Analysis', 'Government', 'Knowledge', 'Test', 'Launches', 'Cost', 'Power', 'Along', 'Several', 'Would', 'Thank', 'Senior', 'Corporate', 'Maybe', 'Growth', \n",
    "    'Industry', 'Insurance', 'Head', 'Institute', 'Questions', 'Analytics', 'Global', 'City', 'State', 'Myer', 'United', 'Phase', 'Lastly', 'Mexico', 'Big', 'Financial', 'Have', 'Getting', #Myer\n",
    "    'Online', 'Start', 'Services', 'Figure', 'America', 'Apart', 'Testing', 'Case', 'Works', 'Resources', 'Lack', 'Science', 'Third', 'Kitchen', 'Microsoft', 'Plan', 'Knowing', 'Mobile', #Case\n",
    "    'Throughout', 'Define', 'Europe', 'Clearly', 'Similarly', 'Any', 'Clients', 'Ultimately', 'Among', 'Doing', 'Perhaps', 'Designers', 'Stage', 'Regarding', 'Afterwards',\n",
    "    'Work', 'Executive', 'Visualizing', 'Generally', 'Strategy', 'Seeing', 'Take', 'Support', 'Day', 'Whenever', 'Wows', 'Stakeholders', 'Personas', 'Nevertheless', 'Solution', 'Again', \n",
    "    'Consequently', 'Performance', 'African', 'Center', 'Concept', 'Banking', 'Part', 'Risk', 'Together', 'Indeed', 'Children', 'Non', 'Does', 'Yet', 'Reﬂection', 'Building', 'Did', 'Meyer', #Meyer Did\n",
    "    'Specifically', 'Conclusion', 'Transformation', 'College', 'Co', 'Focus', 'Right', 'Keeping', 'Under', 'National', 'Smart', 'Prototype', 'Introduction', 'Teams', 'Personal', 'International', \n",
    "    'Something', 'Creative', 'Computer', 'Users', 'American', 'Personally', 'Current', 'Drawing', 'European', 'Sharing', 'Care', 'Peru', 'Point', 'Few', 'Out', 'Consumer', 'Colombia', #Few\n",
    "    'Eventually', 'Virtual', 'Exercise', 'Think', 'Build', 'Nowadays', 'Cloud', 'Zoom', 'Covid', 'Criteria', 'Teachers', 'Better', 'March', 'Everything', 'Note', 'States', \n",
    "    'Things', 'End', 'Master', 'Delivery', 'Explain', 'Actually', 'Hypothesis', 'Whether', 'Develop', 'Main', 'Below', 'China', 'Web', 'Public', 'Parents', 'Solutions', 'Consultant',\n",
    "    'Date', 'Members', 'Lead', 'Excel', 'Core', 'Everybody', 'Liedtka', 'Production', 'Results', 'Administration', 'Until', 'Media', 'Level', 'Very', 'Workshop', 'About', 'Chinese', #Liedka\n",
    "    'Prior', 'Apple', 'Likewise', 'Real', 'Self', 'Emotional', 'Please', 'Platform', 'East', 'Small', 'Impact', 'September', 'Get', 'Elements', 'Spanish', 'Singapore', 'Diagram', 'Vision', #Apple\n",
    "    'Ministry', 'Canvas', 'World', 'Pictures', 'Learn', 'Wow', 'Store', 'Three', 'Ask', 'Back', 'October', 'Inc.', 'Latin', 'Food', 'Previously', 'Simple', 'Industrial', 'Ideation', 'Identifying', \n",
    "    'Australia', 'December', 'Add', 'Depending', 'Creation', 'Subsequently', 'Lean', 'Prototyping', 'Teaching', 'I´m', 'Mindmapping', 'January', 'Central', 'Engineer', 'Improve', 'Go', 'Enterprise', \n",
    "    'Launching', 'Medical', 'Applications', 'Pakistan', 'Perspective', 'Red', 'Manufacturing', 'Intelligence', 'Chief', 'Background', 'Nigeria', 'Resource', 'Understand', 'Miro', # Red Miro\n",
    "    'Korean', 'Machine', 'Task', 'Employee', 'Keep', 'Developing', 'Increase', 'Number', 'Staff', 'Had', 'Statement', 'Assumptions', 'Soon', 'Directors', 'President', 'Problems', 'Section', #Had Soon\n",
    "    'Interviewee', 'Professional', 'Designer', 'Strategic', 'Coming', 'Indonesia', 'Easy', 'Latino', 'Amazon', 'Event', 'Analyst', 'Peace', 'Healthcare', 'Meeting', 'Review', 'Security', \n",
    "    'Unit', 'Check', 'Solving', 'Centre', 'North', 'Earlier', 'Live', 'Legal', 'Interviews', 'Consumers', 'Home', 'French', 'Need', 'Companies', 'Persona', 'Question', 'Mexican', #Moore\n",
    "    'November', 'Safety', 'See', 'Obviously', 'Women', 'York', 'Almost', 'Imagine', 'Graded', 'Italy', 'Additional', 'Trying', 'Flow', 'Shared', 'Giving', 'Paper', 'Generation', 'Pre', #----\n",
    "    'Best', 'Supply', 'Put', 'Unlike', 'Committee', 'Category', 'Partner', 'Account', 'Credit', 'Adding', 'Career', 'References', 'Educational', 'Price', 'Author', 'Sprint', 'Was', 'Fast', \n",
    "    'Implementation', 'Providing', 'Projects', 'Products', 'Job', 'Systems', 'Images', 'Retail', 'Still', 'Officer', 'Normally', 'Farmers', 'Content', 'Experts', 'Study', 'Argentina',  #Job\n",
    "    'Multiple', 'Play', 'Lego', 'Find', 'Open', 'Picture', 'Luckily', 'Infrastructure', 'Field', 'Cases', 'Spain', 'Session', 'Fortunately', 'Description', 'Reference', 'Share', 'Examples', \n",
    "    'Could', 'Improvement', 'Youth', 'German', 'Little', 'Oil', 'Free', 'August', 'Great', 'Tech', 'Leaders', 'Interface', 'Target', 'Philippines', 'Whilst', 'Loan', 'Telecom', \n",
    "    'Submitted', 'Bringing', 'Consulting', 'Similar', 'Presenting', 'Minimum', 'Asian', 'Should', 'Graphic', 'Russia', 'Foundation', 'Finding', 'Germany', 'Same', 'Discussion', 'Relationship', \n",
    "    'Coffee', 'Organizational', 'Chart', 'Network', 'Pilot', 'Database', 'Subject', 'Talent', 'Private', 'Meanwhile', 'Surprisingly', 'Mechanical', 'Notes', 'Automation', 'Give', 'Selected', \n",
    "    'Various', 'Alternative', 'Distribution', 'Logistics', 'Shop', 'Community', 'Hiring', 'Individual', 'Interviewer', 'Write', 'Goals', 'Always', 'Typically', 'Reality', 'Club', 'Middle', \n",
    "    'Thereafter', 'Hyderabad', 'Ideate', 'Lab', 'Activities', 'Studies', 'Voice', 'Workshops', 'Agriculture', 'Italian', 'Engagement', 'Around', 'Startup', 'Requirements', 'Investment', \n",
    "    'Twitter', 'Culture', 'Act', 'Game', 'Plus', 'Artificial', 'Scope', 'Compliance', 'Friday', 'Structure', 'Up', 'Gas', 'Segment', 'Rubric', 'Brands', 'Communications', 'Title', 'Top', \n",
    "    'Existing', 'Area', 'Action', 'Major', 'Effective', 'Wi', 'Fi', 'Presentation', 'Air', 'Word', 'Activity', 'Provide', 'Thirdly', 'Six', 'Stakeholder', 'Draw', 'Travel', 'Saudi', \n",
    "    'Email', 'Classification', 'Banks', 'Procurement', 'Issues', 'France', 'Simply', 'Method', 'Revenue', 'Physical', 'Programme', 'List', 'Waste', 'Failure', 'Putting', 'Korea', 'Image', \n",
    "    'Energy', 'Otherwise', 'Electronics', 'Campus', 'Specific', 'Scientific', 'Decision', 'British', 'Skype', 'Maintenance', 'Beyond', 'Interestingly', 'Engaging', 'Annual', 'Long', 'Visualizations', \n",
    "    'Show', 'None', 'Environment', 'Fig', 'Root', 'Net', 'Operational', 'Goal', 'Consultants', 'Conducting', 'Monday', 'Industries', 'Present', 'Steps', 'Language', 'Audience', 'Gallery', 'Try', \n",
    "    'Improving', 'Set', 'E.g.', 'Diary', 'Kanban', 'Priority', 'Creativity', 'Showing', 'Faculty', 'Age', 'Select', 'Managing', 'Architecture', 'Majority', 'Russian', 'Medium', 'Execution', \n",
    "    'Material', 'Ideally', 'Operating', 'Asking', 'Implementing', 'Result', 'Engineers', 'Moscow', 'Policy', 'Patients', 'Scenario', 'Satisfaction', 'Commercial', 'Money', 'Budget', 'Limited', \n",
    "    'Control', 'Tableau', 'Inc', 'Assumption', 'Straw', 'Institutions', 'Regional', 'Much', 'Writing', 'Allow', 'Made', 'Greek', 'Success', 'February', 'Above', 'Secondary', 'Pin', 'Processing', \n",
    "    'Important', 'Sector', 'Expert', 'Recruitment', 'Egypt', 'Mostly', 'Christmas', 'Special', 'Benefits', 'Accordingly', 'Alternatively', 'Help', 'Others', 'Tell', 'Water', 'Ethnography', \n",
    "    'Line', 'Selling', 'Talking', 'Viable', 'Requirement', 'Approach-', 'Café', 'Traditional', 'Used', 'Person', 'Japanese', 'Mindset', 'Reason', 'Cross', 'Organisation', 'The\\u200b', \n",
    "    'Urban', 'Essentially', 'Academic', 'Council', 'Report', 'Less', 'Clinic', 'Rapid', 'Excellence', 'Request', 'Four', 'Ethnographic', 'Confidential', 'Changing', 'Traditionally',\n",
    "    'Innovative', 'Visualize', 'Table', 'Selection-', 'Application-', 'Planners', 'Japan', 'External', 'Thailand', 'Diagrams', 'Canada', 'Acquisition', 'Initial', 'Coaching',\n",
    "    'Choose', 'Pandemic', 'Executives', 'Reading', 'Agents', 'Skills', 'Agency', 'Local', 'Coronavirus', 'Too', 'Turkey', 'Cause', 'Payment', 'Order', 'Saturday', 'Collaboration',\n",
    "    'Assessment', 'Association', 'Survey', 'Automotive', 'Objective', 'Role', 'Proposition', 'Mental', 'Outcome', 'Discovery', 'Jamboard', 'Entrepreneurship', 'Kenya', 'Plant', 'Portal', \n",
    "    'Groups', 'Lunch', 'Remember', 'Built', 'Nonetheless', 'Involving', 'Higher', 'Matter', 'Know', 'Individuals', 'Created', 'Critical', 'Short', 'Interactive', 'Submission', 'Direct', \n",
    "    'Videos', 'Needless', 'Academy', 'Accounting', 'Definitely', 'Sustainable', 'Vendor', 'Sciences', 'Whatever', 'Integration', 'Whereas', 'Suddenly', 'Leading', 'Defining', \n",
    "    'Family', 'Focusing', 'Started', 'Gathering', 'Identified', 'Call', 'Covid19', 'Bangladesh', 'Workplace', 'Cyber', 'Increased', 'I´ve', 'Operation', 'Pension', 'Context', 'Hearing', \n",
    "    'Evaluation', 'Including', 'Applied', 'Materials', 'Chile', 'Avoid', 'Ecuador', 'Society', 'Asset', 'Augmented', 'Issue', 'Large', 'Continuous', 'Force', 'Assistant', 'Governance', \n",
    "    'Mural', 'Standard', 'Positive', 'Child', 'Card', 'Methodology', 'Setting', 'Honestly', 'Sometime', 'Pharmaceutical', 'Consider', 'Blue', 'Competition', 'Helps', 'Reflecting', 'Ghana', \n",
    "    'Library', 'Sourcing', 'Class', 'Zone', 'Schools', 'Read', 'Multi', 'Hopefully', 'Exploration', 'Onboarding', 'Complex', 'Universidad', 'Room', 'Bachelor', 'Available', 'Meetings', \n",
    "    'Firm', 'Reflections', 'Trust', 'Shortly', 'Fortune', 'Collecting', 'Transport', 'Sale', 'Index', 'Clean', 'Explaining', 'Methods', 'Purpose', 'Solar', 'Netherlands', 'Awareness', \n",
    "    'Aside', 'Usage', 'Kids', 'Selecting', 'Studying', 'Interview', 'Python', 'Ensure', 'Funds', 'Soldier', 'Columbia', 'Adobe', 'Illustrator', 'Implement', 'Electric', 'Demand', 'Score', \n",
    "    'Efficiency', 'Half', 'Approval', 'Needs', 'Western', 'Android', 'Soft', 'Matrix', 'Book', 'Associate', 'Events', 'Dubai', 'Aramco', 'Offer', 'Changes', 'Access', 'Supplier', 'Common', \n",
    "    \n",
    "    'Primary', 'Eastern', 'Coach', 'Choosing', 'Conduct', 'Branches', 'Technique', 'Helping', 'History', 'Ex', 'Combining', 'Search', 'Reduce', 'Mother', 'Speed', 'Physics', 'Doctors', \n",
    "    'Region', 'Responsibility', 'Grade', 'Learnings', 'Capital', 'Stress', 'Advertising', 'Establishing', 'Black', 'Delhi', 'Front', 'Brazilian', 'Greece', 'Face', 'Tree', 'Professionals', \n",
    "    'Sustainability', 'Communicating', 'Radio', 'Estate', 'Starbucks', 'Serious', 'Fundraiser', 'Presentations', 'Topic', 'Establish', 'Hello', 'Youtube', 'Sketching', 'Discover', 'Laundry', \n",
    "    'Reports', 'Humans', 'Techniques', 'Document', 'Departments', 'Early', 'Active', 'Sample', 'Basis', 'Already', 'Place', 'Till', 'London', 'Five', 'Experimentation', 'Immediately', 'Branch', \n",
    "    'Housing', 'Designs', 'Construction', 'Observation', 'Pitch', 'Look', 'Carbon', 'Gender', 'Return', 'Fashion', 'Lessons', 'Space', 'Words', 'Naturally', 'Specially', 'Uber', 'Probably', \n",
    "    'Confluence', 'Talk', 'Increasing', 'Member', 'Year', 'Divine', 'Manual', 'Connecting', 'Uruguay', 'Leads', 'Previous', 'Reporting', 'Campaign', 'Particularly', 'Advantages', 'Trade', \n",
    "    'Napkin', 'Opportunities', 'Prototypes', 'Analyzing', 'Drive', 'Press', 'Sub', 'Allowing', 'Photoshop', 'Storyboard', 'Answers', 'Kingdom', 'Between', 'Conference', 'Equipment', 'Proposal', \n",
    "    'Switzerland', 'Indicators', 'Nations', 'Airport', 'Objectives', 'Gaining', 'Division', 'Compared', 'Scale', 'Motivation', 'Shopping', 'Website', 'Answer', 'Clear', 'Patient', 'Rules', \n",
    "    'Balance', 'Vehicle', 'True', 'Reduction', 'Engine', 'Maersk', 'Approaches', 'Certain', 'Walk', 'Opening', 'Hotel', 'Related', 'Interaction', 'Challange', 'Everyday', 'Poor', 'Situation', \n",
    "    'Schedule', 'Mainly', 'Nothing', 'Theory', 'Hospital', 'Fail', 'Buyer', 'Type', 'Interior', 'Device', 'Principal', 'Portfolio', 'Promoter', 'Agencies', 'Nature', 'Labor', 'Costs', 'México', \n",
    "    'Preparing', 'Electronic', 'Union', 'Average', 'Desing', 'Natural', 'Done', 'Quito', 'Logo', 'Payroll', 'Slowly', 'Invite', 'Advanced', 'Identification', 'Cards', 'Win', 'Enterprises', \n",
    "    'Tuesday', 'Malaysia', 'Park', 'Einstein', 'Literacy', 'Basic', 'Brainstorm', 'Generate', 'Engage', 'Thereby', 'Resolution', 'Anniversary', 'Buy', 'Studio', 'Developer', 'Movie', 'Corporation', \n",
    "    'Lecture', 'Were', 'Federal', 'Quite', 'District', 'Past', 'Plastic', 'Break', 'Metro', 'Framework', 'Color', 'Breaking', 'Experiences', 'Facility', 'Anyway', 'Lots', 'Lower', 'Charts', \n",
    "    'Definition', 'Gain', 'Quick', 'Sports', 'Emergency', 'Math', 'Musk', 'Dev', 'Fuel', 'Magazine', 'Tests', 'Application:-', 'Apps', 'Incubator', 'Really', 'Speaking', 'Form', 'Masters', \n",
    "    'Town', 'Remote', 'Analyze', 'Facing', 'Exam', 'Total', 'Juniors', 'Calendar', 'Wireless', 'Headphones', 'Robotics', 'Molybdenum', 'Graphics', 'Bengali', 'Modern', 'Economy', \n",
    "    'Relations', 'Months', 'Experiment', 'Patent', 'Property', 'Values', 'Record', 'Technological', 'Exchange', 'Perfect', 'Exploring', 'Electrical','Prepared', 'Normal', 'Continuing', 'Output', \n",
    "    'Coordinator', 'Site', 'Developers', 'Billing', 'Rate', 'Optimization', 'Retailers', 'Inspired', 'Photography', 'Bogotá', 'Coca', 'Cola', 'Elderly', 'Parks', 'Drilling', 'Spending', \n",
    "    'Location', 'Rule', 'View', 'Birthday', 'Wealth', 'Points', 'Monthly', 'Provider', 'Functional', 'Unique', 'Years', 'Mathematics', 'Statistics', 'Contact', 'Gradually', 'Recent', 'Organic', \n",
    "    'Alumni', 'Apply', 'Volunteers', 'Toastmasters', 'Degree', 'Chemical', 'Myself', 'Measure', 'Netflix', 'Secret', 'Anxiety', 'Operators', 'Incorporating', 'Cybersecurity', 'Narrative', \n",
    "    'Associates', 'Publishing', 'Accounts', 'Traffic', 'Selection:-', 'Packaging', 'Possible', 'Follow', 'Release', 'Might', 'Input', 'Addressing', 'Commerce', 'Include', 'Chicago', 'Features', \n",
    "    'Source', 'Circle', 'Internally', 'Audio', 'Arabic', 'Admin', 'Reviewing', 'Projective', 'Discussing', 'Specialist', 'Visualising', 'Wellness', 'Mumbai', 'Carriers', 'Bitcoin', 'Country', \n",
    "    'Promoting', 'Transfer', 'Income', 'Maintain', 'Introducing', 'Municipality', 'Music', 'Parts', 'Profit', 'Provided', 'Counseling', 'Architect', 'Inventory', 'Journal', 'Manage', 'Response', \n",
    "    'Guide', 'Improved', 'Eliminate', 'Rural', 'Stream', 'Latinos', 'Advantage', 'Tax', 'Brief', 'Inputs', 'Facilities', 'Employer', 'Agreement', 'Father', 'Whole', 'Semester', 'Moderate', 'Discuss', \n",
    "    'Full', 'Proof', 'Box', 'Discussions', 'Learners', 'Administrative', 'Northern', 'Resolutions', 'Learner', 'Recovery', 'Apparently', 'Plot', 'Promote', 'Mission', 'Visit', 'Uganda', 'Etc', \n",
    "    'Prevention', 'Alzheimer', 'Leader', 'Peruvian', 'Egyptian', 'Ambassador', 'Patterns', 'Sensitivity', 'Double', 'House', 'Assistants', 'Leaning', 'Branding', 'Salesforce', 'Clinical', 'I`m', \n",
    "    'Indians', 'Challenge-', 'Symbian', 'Population', 'Meaning', 'Chapter', 'Originally','Equity', 'Analysts', 'Regardless', 'Historically', 'Jira', 'Workflow', 'Processes', 'Admissions', \n",
    "    'Pressure', 'Collaborative', 'Efficient', 'Entrepreneur', 'Technologies', 'Iran', 'Environmental', 'Windows', 'Drug', 'Confirming', 'Org', 'Identity', 'Micro', 'Eg', 'Purchase', 'Classroom', \n",
    "    'Vendors', 'Meet', 'Interesting', 'Closing', 'Agent', 'Institution', 'Capgemini', 'Unless', 'Restaurant', 'Demo', 'Elementary', 'Oblasts', 'Lately', 'Economics', 'Loss', 'Practical', 'Decide', \n",
    "    'Observing', 'Riding', 'Tracking', 'Stock', 'Practice', 'Monitoring', 'Typical', 'Renaissance', 'Games', 'Link', 'Founder', 'Unknown', 'Written', 'Electricity', 'Universities', 'Owner', 'Train', \n",
    "    'Arabia', 'Kinect', 'Volume', 'Challenge:\\u200b', 'Arab', 'Retrieved', 'Documentation', 'Strong', 'Earth', 'Madrid', 'Americans', 'Artists', 'Utilizing', 'Availability', 'Azerbaijan', 'Added', \n",
    "    'Sponsorship', 'Separate', 'Entrepreneurs', 'Crafting', 'Outcomes', 'Pharma', 'Terms', 'Feb', 'Provides', 'Actual', 'Transaction', 'Station', 'Performing', 'Narrating', 'Frequently', 'Maximum', \n",
    "    'Induction', 'Potential', 'Integrated', 'Mindmap', 'Print', 'Powerpoint', 'Aires', 'Humidity', 'Tables', 'Whiteboard', 'Representatives', 'Classes', 'Amongst', 'Either', 'Triple', 'Old', 'Paraguay', \n",
    "    'Track', 'Advisor', 'Inside', 'Assembly', 'Round', 'Inspiration', 'Layout', 'Stick', 'Organize', 'Afterward', 'Daily', 'Proper', 'Lesson', 'Pillar', 'Alternatives', 'Somehow', 'Approach:-', 'Sponsors', \n",
    "    'Storyboarding', 'Fourth', 'Facts', 'Workers', 'Organizing', 'Orders', 'Oral', 'Army', 'Psychology', 'Capture', 'Visuals', 'Empathize', 'Collection', 'Complete', 'Businesses', 'Innovate', 'Focused', \n",
    "    'Turning', 'Fintech', 'Domain', 'Framing', 'It´s', 'Hospitals', 'Theater', 'Lockdown', 'Bring', 'Principles', 'Desk', 'Americas', 'Crazy', 'Colleagues', 'Aplication', 'Encourage', 'Improves', \n",
    "    'Civil', 'Never', 'Citizens', 'Domestic', 'Carrier', 'Palestine', 'Cycles', 'Detail', 'Assurance', 'Retention', 'Persian', 'Mens', 'Player', 'Amsterdam', 'Battery', 'Competitors', 'Running', 'Sadly', \n",
    "    'Representative', 'Confidence', 'Prioritization', 'Networks', 'Observe', 'Adventure', 'Cultural', 'Experimenting', 'Strategies', 'Areas', 'Equally', 'Anyone', \n",
    "           \n",
    "    'Advisory', 'Application\\u200b', 'Challenge\\u200b', 'We\\u200b', 'Insight\\u200b', 'Automated', 'Cycle', 'Pick', 'Climate', 'Emirates', 'Sticky', 'Outsourcing', 'Code', 'Govt', 'Profession', 'Disadvantage', \n",
    "    'Adapt', 'Progress', 'Collective', 'Ecosystem', 'Garden', 'Detailed', 'Political', 'Collected', 'Happiness', 'Sony', 'Millennial', 'Towards', 'Concepts', 'Medicine', 'Trello', 'Organisational', 'Channels', \n",
    "    'Repository', 'Listen', 'Alternate', 'Fixed', 'Repeat', 'Fishbone', 'Surveys', 'Briefly', 'Initiative', 'Construct', 'Buenos', 'Undoubtedly', 'Economic', 'Prepare', 'Suite', 'Cancer', 'Audiences', \n",
    "    'Attendees', 'Networking', 'Relevant', 'Inspiring', 'Berlin', 'Innovator', 'Excellent', 'Amount', 'Size', 'Transparency', 'Direction', 'Regulatory', 'Begin', 'Entertainment', 'Printing', 'Regular', 'Disease', \n",
    "    'Simplify', 'Curriculum', 'Insight-', 'Structural', 'Sponsor', 'Memorial', 'Option', 'Importance', 'Beside', 'Hackathon', 'Living', 'Text', 'Representation', 'Documents', 'Forward', 'Contractor', 'Describing', \n",
    "    'Continuity', 'Occasionally', 'Trainings', 'Ladies', 'Factors', 'Neither', 'Centers', 'Diversity', 'Targets', 'Contribution', 'Diabetes', 'Single', 'Venezuela', 'Disconfirming', 'Unicamp', 'Namibia', 'Debit', \n",
    "    'Wireframes', 'Mortgage', 'Period', 'Types', 'Primarily', 'Skill', 'Congressional', 'Collect', 'Backlog', 'Taiwan', 'Sorry', 'Item', 'Realtors', 'Brussels', 'Guys', 'Adult', 'Staying', 'Details', \n",
    "    'Bike', 'Pricing', 'Marketplace', 'Introduce', 'Stores', 'Risks', 'Instructions', 'Courses', 'Phone', 'Preparation', 'Refection', 'Programs', 'Mirror', 'Tourism', 'Pride', 'Questionnaire', 'Trainer', 'Days', \n",
    "    'Forms', 'Hospitality', 'Approach:\\u200b', 'Vietnam', 'Agreements', 'Custom', 'Suppliers', 'Appraisal', 'Methodologies', 'Finances', 'I´ll', 'Nobody', 'Southern', 'Teamwork', 'Reverse', 'Friends', \n",
    "    'Attractive', 'Boards', 'Searching', 'Loyalty', 'Compare', 'Consistent', 'Timeline', 'Condition', 'Regulation', 'Millennials', 'Biological', 'Nigerian', 'Intelligent', 'Experiments', 'Investors', 'Sessions', \n",
    "    'Segments', 'Centered', 'Handling', 'Disaster', 'Productivity', 'Internship', 'Synthesis', 'Ability', 'Relationships', 'Surely', 'Beginning', 'Conflict', 'Incident', 'Assuming', 'Certainly', 'Manufacturer', \n",
    "    'Premium', 'Procedure', 'Blind', 'Light', 'Followed', 'Display', 'Redesigning', 'Dynamic', 'Graph', 'Authenticity', 'Quickly', 'Citizen', 'Graphs', 'Tourists', 'Timely', 'Infographics', 'Animal', 'Competencies', \n",
    "    'Anything', 'Insight:-', 'Buying', 'Forum', 'Opinion', 'Animation', 'Address', 'Requests', 'Distributor', 'Restricted', 'Conducted', 'Character', 'Defined', 'Pixar', 'Items', 'Girls', 'Challenge:-', 'Gentlemen', \n",
    "    'Pattern', 'Deliver', 'Australian', 'Syllabus', 'Successful', 'Thankfully', 'Hypotheses', 'Inspire', 'Template', 'Landscape', 'Summary', 'Discussed', 'Except', 'Ensuring', 'Numerous', 'Simultaneously', 'Offering', \n",
    "    'Innovations', 'Canadian', 'Perception', 'Myanmar', 'Airbnb', 'Essential', 'Launch-', 'Ambiguity', 'Continue', 'Reaction', 'Suppose', 'Purchasing', 'Satellite', 'Guinness', 'Effects', 'Architectural', 'Deutsche', \n",
    "    'Storyboards', 'Festival', 'Probability', 'Photo', 'Feature', 'Belonging', 'Century', 'Flipped', 'Visualizaton', 'Fundraising', 'Improvements', 'Sources', 'Regards', 'Scheduling', 'Opportunity', 'Upgrading',\n",
    "    'Profiling', 'Candidates', 'Slides', 'Flexibility', 'Presently', 'Descriptive', 'Appendix', 'Ancient', 'Opinions', 'Actions', 'Eating', 'Cooking', 'Collectively', 'Shift', 'Bluetooth', 'Perú', 'Honduras', \n",
    "    'Reinforce', 'Laboratory', 'Screen', 'Procedures', 'Highly', 'Proposed', 'Workforce', 'Audit', 'Champions', 'Linked', 'Toolkit', 'Toronto', 'Religion', 'Affinity', 'Leave', 'Accessed', 'Paragraph', 'Contracting', \n",
    "    'Alliance', 'Persons', 'Combine', 'Minimize', 'Enhancing', 'Thesis', 'Growing', 'Comfortable', 'Calculate', 'Sheets', 'Interest', 'Walmart', 'Parking', 'Recognition', 'Paying', 'Highlighting', 'Package', \n",
    "    'Leveraging', 'Constant', 'Effect', 'Welcome', 'Personality', 'Maping', 'Sitting', 'Translate', 'Aproach', 'Models', 'Practically', 'Gather', 'Hours', 'Mobility', 'Walking', 'Counselling', 'Qualitative', 'Scientist', \n",
    "    'Levels', 'Server', 'Topics', 'Easter', 'Filipinos', 'Warehouse', 'Accenture', 'Secretary', 'Expected', 'Approximately', 'Employment', 'Guatemala', 'Diverse', 'Turkish', 'Workspace', 'Presence', 'Barriers',\n",
    "    'Validation', 'Interviewing', 'Standardization', 'Linear', 'Graphical', 'Namely', 'Generating', 'Village', 'Empathise', 'Explanation', 'Temp', 'Frequent', 'Dandruff', 'Untitled', 'Lionsgate', 'Quantitative', \n",
    "    'Mining', 'Scenarios', 'Learnt', 'Concurrently', 'Airlines', 'Colombian', 'Faster', 'Intellectual', 'Wherever', 'Narration', 'Referring', 'Connections', 'Female', 'Quiz', 'Significant', 'Strengths', \n",
    "    'Photographs', 'Researching', 'Jogging', 'Leisure', 'Attention', 'Nearly', 'Possibly', 'Evaluating', 'Computing', 'Connect', 'Caregivers', 'Possibilities', 'Observations', 'Periodic', 'Museum', \n",
    "    'Glucose', 'Reduced', 'Practices', 'Robot', 'Meteorological', 'Smartphone', 'Flexible', 'Empower', 'Ukraine', 'Upcoming', 'Ecobrick', 'Bundle', 'Believe', 'Statements', 'Guest', 'Selection\\u200b', 'Trends', \n",
    "    'Millions', 'Prepaid', 'Distributors', 'Compact', 'Coupled', 'Defence', 'Faced', 'Enabling', 'Investing', 'Importantly', 'Complexity', 'Enough', 'Elevator', 'Deposit', 'Thought', 'Encourages', 'Speaker', \n",
    "    'Parent', 'Accessibility', 'Promotion', 'Tasks', 'Lebanon', 'Raising', 'Journeys', 'Countries', 'Prefect', 'Racing', 'Geodesic', 'Untold', 'Soldiers', 'Pharmacy', 'Essay', 'Charity', 'Artist', 'Phases', \n",
    "    'Temporary', 'Computers', 'Organized', 'Smartphones', 'Gained', 'Differentiation', 'Visually', 'Analytical', 'Telecommunications', 'Africans', 'Respect', 'Duration', 'Durable', 'Length', 'Feedbacks', \n",
    "    'Participation', 'Constantly', 'Informal', 'Formal', 'Helpdesk', 'Regularly', 'Presented', 'Lecturer', 'Missing', 'Obstacles', 'Counselor', 'Individually', 'Chairman', 'Drawings', 'Scaling', 'Modelling', \n",
    "    'Revolution', 'Selection:\\u200b', 'Desktop', 'Visualized', 'Institutional', 'Payments', 'Competitive', 'Savings', 'Visibility', 'Decisions', 'Sprints', 'Requires', 'Affordable', 'Application:\\u200b', 'Mentor', \n",
    "    'Incorrect', 'Facilitator', 'Behavioural', 'Sanitation', 'Librarians', 'Shipping', 'Someone', 'Functions', 'Outline', 'Ltd', 'Visits', 'Logic', 'Petrol', 'Pizza', 'Boomers', 'Therefor', 'Buyers', 'Certificate', \n",
    "    'Crucial', 'Digging', 'Programming', 'Responsible', 'Bucket', 'Weight', 'Interactions', 'Click', 'Therapists', 'Advertisement', 'Boarding', 'Wordpress', 'Desirability', 'Viability', 'Robotic', 'Rising', 'Tried', \n",
    "    'Servers', 'Certification', 'Saving', 'Steering', 'Secure', 'Literature', 'Grocery', 'Clusters', 'Guided', 'Hydroponics', 'I’am', 'Operator', 'Volunteer', 'Export', 'Utility', 'Motivations', 'Responses', \n",
    "    'Centric', 'Distance', 'Sweden', 'Locality', 'Belgium', 'Transportation', 'Interpretation', 'Marketers', 'Mapped', 'Letter', 'Physicians', 'Videocall', 'Realization', 'Relevance', 'Dashboards', 'Textile', 'Minimal', \n",
    "    'Incubation', 'Multinational', 'Anywhere', 'Selfcare', 'Volkswagen', 'Eliminating', 'Redesign', 'Thoughts', 'Surcharge', 'Parties', 'Express', 'Conversion', 'Employing', 'Oftentimes', 'Negative', 'Algorithm', \n",
    "    'I`ve', 'Desired', 'Overview', 'Facilitating', 'Deployment', 'Intern', 'Standing', 'Adopting', 'Scalability', 'Childhood', 'Effectively', 'Petroleum', 'Recommendations', 'Comparing', 'Worker', 'Cooperation', \n",
    "    'Remembering', 'Investigate', 'Painting', 'Assignment-', 'Approach\\u200b', 'Designed', 'Frankly', 'Coordination', 'Random', \n",
    "#                'I', 'A', 'Mr', 'Student'\n",
    "    'Thinks', 'Protocol', 'Hindu', 'Evolution', 'Pyramid', 'Financing', 'Docs', 'Status', 'Says', 'Land', 'Certified', 'Gym', 'Linux', 'Hear', 'Tier', 'Plastics', 'Series', 'Farm', 'Islamic', 'Hands', 'Authority', \n",
    "    'Labs', 'Odyssey', 'Contract', 'Enjoy', 'Path', 'Vol', 'Dear', 'Raspberry', 'Norwegian', 'Capacity', 'Ramadan', 'Washington', 'Thinkers', 'Minister', 'Behind', 'Hold', 'Foreign', 'Tabs', 'Tv', 'Ankara', 'Truly', \n",
    "    'Historical', 'Seafood', 'Jewellery', 'Winner', 'Advance', 'Provision', 'Qtr', 'Poland', 'Known', 'England', 'Seminar', 'Ground', 'Ring', 'Aiming', 'Sign', 'Gaming', 'Send', 'Filter', 'Diet', 'Evidence', \n",
    "    'Forbes', 'Chamber', 'Denmark', 'Aids', 'Coaches', 'Unified', 'Church', 'Guidance', 'Invest', 'Tanzania', 'Temperature', 'Names', 'Us', 'Sounds', 'Weekend', 'Pivot', 'Version', 'Swiss', 'Factor', 'Cooperative', \n",
    "    'Tribal', 'Belgian', 'Arduino', 'Finland', 'Balloons', 'Iron', 'Landing', 'Speech', 'Dream', 'Tour', 'Specialists', 'Humanity', 'Difficult', 'Editorial', 'Sketches', 'Logistic', 'Rotterdam', 'Bayer', 'Malta', \n",
    "    'Benefit', 'Diploma', 'Million', 'Ericsson', 'Junk', 'Thinker', 'Muslim', 'Seller', 'Style', 'Traveller', 'Pass', 'Skin', 'Bear', 'Guess', 'Saved', 'Pages', 'Twice', 'Wrong', 'Captain', 'Retailer', 'Austria', \n",
    "    'Upper', 'Memory', 'Caribbean', 'Roaming', 'Nurses', 'Male', 'Timing', 'Georgian', 'Prospect', 'Depression', 'Rowling', 'Kindergarten', 'Fears', 'Blessing', 'Cotton', 'Waterfall', 'Habitat', 'Retro', 'Aware', \n",
    "    'Bags', 'Milton', 'Margin', 'Newspaper', 'Comfort', 'Lisbon', 'Portugal', 'Spotify', 'Alexandria', 'Minor', 'Tokyo', 'Kindly', 'Bolivia', 'Greater', 'Flight', 'Limit', 'Buffet', 'Nacional', 'Israel', 'Closed', \n",
    "    'Cambridge', 'Bias', 'Valenzuela', 'Alibaba', 'Wild', 'Batch', 'Occasion', 'Guaranteed', 'Honest', 'Ordinary', 'Mutual', 'Kings', 'Block', 'Blank', 'Vertical', 'Danish', 'Shanghai', 'Scatter', 'Exhibit', \n",
    "    'Night', 'Programmer', 'Camera', 'Adriatic', 'Planet', 'Battle', 'Mask', 'Zurich', 'Cologne', 'Incomplete', 'Solve', 'Seven', 'Scandinavian', 'Grandmother', 'Sleep', 'Emotion', 'Salary', 'Nepal', 'Christians', \n",
    "    'Sense', 'Nederland', 'Whatsapp', 'Linking', 'Move', 'Santa', 'Sellers', 'Ranking', 'Acceptance', 'Manchester', 'Difference', 'Original','Naming', 'Drink', 'Tips', 'Lithuania', 'Hoping', 'Winning', 'Radiant',\n",
    "    'Tracks', 'Minecraft', 'Script', 'Installation', 'Bearing', 'Technician', 'Pause', 'Mountain', 'Genetic', 'Stand', 'Older', 'Silicon', 'Appeal', 'Flawless', 'Surprise', 'Polish', 'Embassy', 'Corp', 'Crown', 'Aided', \n",
    "    'Monster', 'Meter', 'Reserve', 'Candidate', 'Info', 'Trend', 'Insta', 'Spread', 'Tension', 'Fresh', 'Cambodia', 'Eisenhower', 'Wifi', 'Delay', 'Poster', 'Wait', 'Travelling', 'Contractors', 'Cisco', 'Dirt', \n",
    "    'Messaging', 'Reach', 'Intro', 'Projekt', 'Tasting', 'Leeds', 'Annex', 'Dictator', 'Futuristic','Famous', 'Swedish', 'Taxi', 'Obj', 'Signal', 'Mobiles', 'Estimate', 'Pros', 'Extensions', 'Colorado',  \n",
    "    'Determine', 'Found', 'Confident', 'Merchant', 'Revelation', 'Happily', 'Pack', 'Pool', 'Grateful', 'Volvo', 'Impossible', 'Chapters', 'Trigger', 'Pixel', 'Wonderland', 'Vivid', 'Yesterday', 'Commodity', 'Trading', \n",
    "    'Younger', 'Southeast', 'Mention', 'Buddhist', 'Fashionable', 'Luxury', 'Cruise', 'Labour', 'Accept', 'Machinery', 'Covenant', 'Billion', 'Melbourne', 'Biblioteca', 'Driven', 'Caution', 'Hobby', 'Anton', 'Holland', \n",
    "\n",
    "#     'Gray', 'West', 'Sales', 'White', 'Green', 'Young', 'Asia', 'Brown', 'Moore',\n",
    "#     'Student', 'Mars', 'May', 'Asia', 'Low', 'Virginia',\n",
    "]\n",
    "\n",
    "stop_wordsY = ['The', 'We', 'This', 'In', 'It', 'Challenge', 'As', 'What', 'Mind', 'For', 'After', 'Design', 'So', 'Our', 'To', 'They', 'My', 'And', 'When', 'By', \n",
    "    'Then', 'If', 'With', 'There', 'But', 'One', 'At', 'Since', 'Also', 'Tool', 'While', 'How', 'Once', 'That', 'Each', 'During', 'All', 'From', 'Using', 'Now', \n",
    "    'Next', 'On', 'First', 'Innovation', 'Some', 'Business', 'Example', 'He', 'Story', 'You', 'Most', 'India', 'Visual', 'People', 'Company', 'Many', 'Thus', 'Based', \n",
    "    'Hence', 'Assignment', 'She', 'Even', 'Map', 'Project', 'An', 'Before', 'Here', 'Every', 'Team', 'Given', 'Is', 'Data', 'Why', 'Sales', 'Due', 'Not', 'Being', 'Use', \n",
    "    'Journey', 'Telling', 'English', 'No', 'Week', 'Process', 'New', 'Working', 'Future', 'Well', 'Value', 'More', 'Internet', 'Second', 'Do', 'Moving', 'According', \n",
    "    'Service', 'Human', 'Final', 'Good', 'Firstly', 'Post', 'Key', 'Like', 'Other', 'Step', 'Just', 'School', 'Idea', 'Who', 'Time', 'Overall', 'Indian', 'Let', 'Health', \n",
    "    'Chain', 'App', 'Finance', 'Both', 'Maps', 'Make', 'Group', 'Later', 'Such', 'Video', 'Your', 'Bank', 'Despite', 'Board', 'Page', 'Its', 'Education', 'Agile', 'Two', \n",
    "    'Create', 'Can', 'Going', 'Name', 'Experience', 'Last', 'Peer', 'Brand', 'Ideas', 'Brazil', 'Over', 'Change', 'Quality', 'Tools', 'Only', 'General', 'Student',  #Student\n",
    "    'Office', 'Director', 'South', 'Gray', 'Technical', 'Her', 'Client', 'Are', 'Market', 'Course', 'Yes', 'High', 'Me', 'Life', 'Model', 'Congress', 'Be', 'Rather', 'Africa', \n",
    "    'Or', 'Analysis', 'Knowledge', 'Test', 'Cost', 'Power', 'Along', 'Thank', 'Senior', 'Maybe', 'Growth', 'Head', 'City', 'State', 'Mars', 'Myer', 'United', 'Phase', 'Mexico', \n",
    "    'Big', 'Have', 'Getting', 'Start', 'Figure', 'America', 'Apart', 'Works', 'Lack', 'Science', 'Third', 'Kitchen', 'Plan', 'Mobile', 'Define', 'Europe', 'Asia', 'Any', 'Low', \n",
    "    'Among', 'Doing', 'Stage', 'Regarding', 'Work', 'Seeing', 'Take', 'Day', 'Again', 'Performance', 'African', 'Center', 'Concept', 'Part', 'Risk', 'Together', 'Children', \n",
    "    'Non', 'Does', 'Yet', 'Building', 'Did', 'Meyer', 'Transformation', 'College', 'Co', 'Focus', 'Right', 'Keeping', 'National', 'Smart', 'American', 'Current', 'Drawing', \n",
    "    'Care', 'Peru', 'Point', 'Few', 'Out', 'Build', 'Cloud', 'Professor', 'Zoom', 'Teachers', 'Better', 'March', 'Note', 'States', 'End', 'Master', 'Delivery', \n",
    "    'Develop', 'Main', 'Below', 'China', 'Web', 'Public', 'Brown', 'Parents', 'Date', 'Members', 'Lead', 'Excel', 'Core', 'Green', 'Liedtka', 'Media', 'Level', \n",
    "    'Very', 'Chinese', 'Prior', 'Apple', 'Real', 'Self', 'Platform', 'East', 'Small', 'Impact', 'September', 'Get', 'Elements', 'Spanish', 'Singapore', 'Canvas', 'World', \n",
    "    'Learn', 'Wow', 'Store', 'Three', 'Ask', 'Back', 'October', 'Latin', 'Simple', 'Industrial', 'Australia', 'December', 'Add', 'Lean', 'January', 'Central', 'Engineer', \n",
    "    'Go', 'Pakistan', 'Intelligence', 'Chief', 'Nigeria', 'Korean', 'Machine', 'Task', 'Keep', 'Increase', 'Number', 'Staff', 'Had', 'Soon', 'President', 'Problems', 'Section', \n",
    "    'Coming', 'Easy', 'Latino', 'Amazon', 'Peace', 'Meeting', 'Check', 'Centre', 'North', 'Live', 'Moore', 'Legal', 'Home', 'French', 'Need', 'Persona', 'Mexican', 'November', \n",
    "    'See', 'Women', 'York', 'Imagine', 'Flow', 'Paper', 'Pre', 'Best', 'Supply', 'Put', 'Partner', 'Credit', 'Career', 'Price', 'Author', 'Was', 'Fast', 'Job', 'Still',\n",
    "    'Officer', 'Farmers', 'Content', 'Study', 'Argentina', 'Play', 'Lego', 'Find', 'Open', 'Field', 'Cases', 'Spain', 'Session', 'Share', 'German', 'San', 'Little', 'Young', \n",
    "    'Oil', 'Free', 'August', 'Great', 'Tech', 'Leaders', 'Target', 'Loan', 'Consulting', 'Similar', 'Minimum', 'Should', 'Graphic', 'Russia', 'Finding', 'Germany', 'Same', \n",
    "    'Coffee', 'Chart', 'Pilot', 'Subject', 'Talent', 'Private', 'Automation', 'Give', 'Selected', 'Various', 'Shop', 'Hiring', 'Write', 'Club', 'Middle', 'Lab', 'Voice', \n",
    "    'Around', 'Startup', 'Culture', 'Act', 'Game', 'Plus', 'Scope', 'Friday', 'Up', 'Gas', 'Brands', 'Title', 'Top', 'Brain', 'Area', 'Action', 'Major', 'Junior', 'Wi', 'Fi', \n",
    "    'Presentation', 'Air', 'Word', 'Costa', 'Six', 'Draw', 'Travel', 'Banks', 'France', 'Simply', 'Method', 'Physical', 'List', 'Waste', 'Korea', 'Image', 'Energy', 'Campus', \n",
    "    'Scientific', 'British', 'Beyond', 'Long', 'Show', 'None', 'Fig', 'Root', 'Net', 'Goal', 'Monday', 'Present', 'Steps', 'Gallery', 'Try', 'Set', 'Creativity', 'Faculty', \n",
    "    'Age', 'Select', 'Majority', 'Russian', 'Medium', 'Execution', 'Material', 'Asking', 'Moscow', 'Policy', 'Patients', 'Money', 'Control', 'Tesla', 'Straw', 'Much', 'Allow', \n",
    "    'Made', 'Greek', 'Success', 'February', 'Above', 'Sector', 'Christmas', 'Special', 'Benefits', 'Tell', 'Water', 'Line', 'Selling', 'Café', 'Traditional', 'Used', 'Person', \n",
    "    'West', 'Japanese', 'Reason', 'Cross', 'Urban', 'Council', 'Less', 'Rapid', 'Beauty', 'Choice', 'Excellence', 'Four', 'Confidential', 'Innovative', 'Visualize', 'Table',\n",
    "    'Japan', 'Thailand', 'Canada', 'Coaching', 'Choose', 'Reading', 'Agents', 'Local', 'Too', 'Turkey', 'Cause', 'White', 'Payment', 'Order', 'Saturday', 'Survey', 'Role', \n",
    "    'Mental', 'Discovery', 'Kenya', 'Plant', 'Portal', 'Law', 'Lunch', 'Built', 'Higher', 'Matter', 'Know', 'Critical', 'Short', 'Videos', 'Accounting', 'Jobs', 'Suddenly', \n",
    "    'Family', 'Call', 'Re', 'Cyber', 'Operation', 'Pension', 'Hearing', 'Chile', 'Avoid', 'Ecuador', 'Asset', 'Large', 'Force', 'Mural', 'Standard', 'Positive', 'Child', 'Card', \n",
    "    'Setting', 'Honestly', 'Consider', 'Blue', 'Helps', 'Ghana', 'Class', 'Zone', 'Schools', 'Read', 'Multi', 'Room', 'Bachelor', 'Firm', 'Trust', 'Fortune', 'Channel', 'God', \n",
    "    'Transport', 'Sale', 'Index', 'Clean', 'Purpose', 'Solar', 'Kids', 'Interview', 'Soldier', 'Columbia', 'Adobe', 'Electric', 'Demand', 'Score', 'Half', 'Needs', 'Western', \n",
    "    'Book', 'Events', 'Rica', 'Offer', 'Changes', 'Access', 'Common', 'Primary', 'Eastern', 'Coach', 'Ex', 'Search', 'Mother', 'Speed', 'Corona', 'Doctors', 'Region', 'Grade', \n",
    "    'Capital', 'Stress', 'Black', 'Delhi', 'Front', 'Greece', 'Face', 'Tree', 'Radio', 'Estate', 'Serious', 'Topic', 'Discover', 'Laundry', 'Early', 'Active', 'Sample', 'Place', #Hong Kong \n",
    "    'Till', 'London', 'Five', 'Branch', 'Alpha', 'Pitch', 'Look', 'Carbon', 'Gender', 'Return', 'Fashion', 'Space', 'Words', 'Uber', 'Probably', 'Talk', 'Member', 'Year', 'Divine', \n",
    "    'Manual', 'Teacher', 'Uruguay', 'Previous', 'Trade', 'Drive', 'Press', 'Sub', 'Bus', 'Kingdom', 'Switzerland', 'Nations', 'Airport', 'Scale', 'Answer', 'Clear', 'Patient', \n",
    "    'Balance', 'Vehicle', 'True', 'Engine', 'Maersk', 'Certain', 'Walk', 'Hotel', 'Challange', 'Everyday', 'Poor', 'Situation', 'Fail', 'Buyer', 'Device', 'Principal', 'Nature',\n",
    "    'Labor', 'Electronic', 'Union', 'Desing', 'Natural', 'Done', 'Quito', 'Logo', 'Payroll', 'Slowly', 'Cards', 'Win', 'Tuesday', 'Malaysia', 'Park', 'Einstein', 'Basic', 'Pay', \n",
    "    'Buy', 'Studio', 'Movie', 'Hi', 'Lecture', 'Were', 'Federal', 'Quite', 'Past', 'Deep', 'Break', 'Metro', 'Color', 'Breaking', 'Anyway', 'Lots', 'Lower', 'Gain', 'Quick', 'Hall', \n",
    "    'Sports', 'Math', 'Musk', 'Dev', 'Fuel', 'Magazine', 'Apps', 'Form', 'Masters', 'Town', 'Remote', 'Juniors', 'Calendar', 'Graphics', 'Bengali', 'Modern', 'Economy', 'Patent', \n",
    "    'Property', 'Mr', 'Record', 'California', 'Perfect', 'Normal', 'Billing', 'Rate', 'Coca', 'Cola', 'Parks', 'Drilling', 'Location', 'Rule', 'View', 'Points', 'Venture', 'Provider', \n",
    "    'Mid', 'Unique', 'Years', 'Contact', 'Than', 'Degree', 'Chemical', 'Secret', 'Happy', 'Anxiety', 'Traffic', 'Ways', 'Packaging', 'Possible', 'Follow', 'Release', 'Might', \n",
    "    'Commerce', 'Chicago', 'Features', 'Aristotle', 'Circle', 'Admin', 'Mumbai', 'Bitcoin', 'Country', 'Maintain', 'Music', 'Parts', 'Profit', 'Journal', 'Guide', 'Off', 'Rural', \n",
    "    'Stream', 'Latinos', 'Tax', 'Brief', 'Agreement', 'Father', 'Moderate', 'Pink', 'Full', 'Proof', 'Box', 'Roam', 'Kit', 'Learners', 'Northern', 'Baker', 'Learner', 'Mission', \n",
    "    'Gen', 'Uganda', 'Etc', 'Leader', 'Peruvian', 'Egyptian', 'Double', 'House', 'Indians', 'Chapter', 'Bad', 'Jira', 'Pressure', 'Iran', 'Windows', 'Drug', 'Org', 'Micro', 'Eg', \n",
    "    'Purchase', 'Zero', 'Meet',\n",
    "    \n",
    "#     'George',\n",
    "]\n",
    "#'Rubric',\n",
    "\n",
    "design_people = {'M. Liedtka', 'Jeanne M.', 'Donald Norman', 'Williemien Brand', 'April Greiman', 'Paul Ralph', 'Bernard Roth', 'Tom Kelley', \n",
    "                 'Carl DiSalvo', 'Jack Ma', 'Ben Matthews', 'Christoph Meinel', 'Marc Benioff', 'Elena Pacenti', \n",
    "                 'Henry Ford', 'Angela Myer', 'Bruce Archer', 'Tom Kelly', 'Brian Lawson', 'Christopher Alexander', \n",
    "                 'Lawrence Erlbaum', 'Temple Grandin', 'Kate Crowley', 'Andrew Carnegie', 'Ron Miriello', 'Tony Buzan', \n",
    "                 'Laura Fletcher', 'Tim Cook', 'Peter Worthy', 'Roger Martin', 'Stephen Draper', 'Sarah Brooks', \n",
    "                 'Lee Vinsel', 'John Jones', 'Sam Walton', 'Bill Burnett', 'Natasha Iskander', 'Tim Ogilvie', \n",
    "                 'Max Wertheimer', 'Brad Smith', 'Ralph Caplan', 'Mathew Duerden', 'Jessica Eldridge', 'John Gero', \n",
    "                 'Dev Patnaik', 'David Gray', 'Lucy Kimbell', 'Robert Rossma', 'Bill Moggridge', 'Frank Chimero', \n",
    "                 'Don Norman', 'Hellen Keller', 'David Kelley', 'Angela Meyer', 'Mary Barra', 'Sunni Brown', 'Tim Chi', \n",
    "                 'Bruce Nussbaum', 'Scott Doorley', 'Dave Evans', 'Daniel Pink', 'William Gordon', 'Patrick Hadley', \n",
    "                 'Marc Stickdorn', 'Janine Reid', 'Matteo Vignoli', 'Rebecca Ackermann', 'Chad Martin', 'Antionette Carroll', \n",
    "                 'Reed Hastings', 'Alex Osborn', 'Saul Bass', 'Jan Chipchase', 'Pete Rowe', 'Robert Curedale', 'Estee Laude', \n",
    "                 'Sharon Carmichael', 'CJ Meadows', 'Larry Page', 'Henry Dreyfuss', 'Dieter Rams', 'Ellen Lupton', 'Charvi Parikh', \n",
    "                 'Skye Doherty', 'MaeLin Levine', 'Daymond John', 'Vijay Kumar', 'Dave Gray', 'Katie Rast', 'Bo Christensen', \n",
    "                 'James Macanufo', 'Robert McKim', 'Jeanne Liedtka', 'Scott Robinson', 'Johansson Sköldberg', 'Warren Buffett', \n",
    "                 'Richard Buchanan', 'Jan Auernhammer', 'Rachel Brozenske', 'Sheryl Sandberg', 'Morris Asimow', 'Brian Chesky', \n",
    "                 'Nate Spees', 'Sharon Boller', 'Regine Gilbert', 'Michael Dell', 'Donald Schön', 'Elon Musk', 'Jim Bagnall', \n",
    "                 'Jeff Weiner', 'Jeremy Myerson', 'Bill Gates', 'Paul Roben', 'Horst Rittel', 'Steve Jobs', 'Alastair Fuad-Luke', \n",
    "                 'John Arnold', 'Karen Hold', 'Linden Ball', 'Janine Toole', 'Nick Kelly', 'Robert Peters', 'Patrick Link', \n",
    "                 'Herbert Simon', 'Albert Einstein', 'Milton Glaser', 'Robin Mathew', 'Alastair Fuad', 'Frederick Brooks', \n",
    "                 'Kees Dorst', 'Richard Florida', 'Matteo Mura', 'Larry Leifer', 'Peter Rowe', 'Don Koberg', 'Uijun Park', \n",
    "                 'Sabeen Hussain', 'Melvin Webber', 'Hasso Plattner', 'Erin Patterson', 'Tim Brown', 'Alan Fletcher', 'Niels Diffrient', \n",
    "                 'Idris Mootee', 'Walt Disney', 'Alexander Osterwalder', 'Michael Lewrick', 'Erik Edgard', 'Edward Bono', 'Frederik Pferdt', \n",
    "                 'Mark Zuckerberg', 'Terry Irwin', 'Michael Beverland', 'Thomas Lockwood', 'Jeff Bezos', 'Alex Osterwalder', 'Joe Gebbia', \n",
    "                 'Patti Dobrowolski', 'Walter, Hall', 'Sarah Wilner', 'Bryan Lawson', 'Pietro Micheli', 'Jakob Schneider', 'Scott Edwards', \n",
    "                 'Alastair Luke', 'Ezio Manzini', 'Daniel Goleman', 'Liz Sanders', 'Kātz Barry', 'Shelley Goldman', 'Richard Branson', \n",
    "                 'Rolf Faste', 'Deborach Szebeko', 'Brian Head', 'Jon Kolko', 'Ralf Speth', 'Nigel Cross', 'Paul Graham', 'Stefan Wiltschnig'}\n",
    "def check_designers(tokens):\n",
    "    token = tokens[0]\n",
    "    tokens_text = token.strip()\n",
    "    next_token = ''\n",
    "    if len(tokens) >= 2:\n",
    "        next_token = tokens[1]\n",
    "    if next_token.strip() != '':\n",
    "        tokens_text += ' ' + next_token.strip()\n",
    "    if tokens_text in design_people:\n",
    "#         print(token, '           ', tokens_text)\n",
    "        return True\n",
    "        \n",
    "    next_next_token = ''\n",
    "    if len(tokens) >= 3:\n",
    "        next_next_token = tokens[2]\n",
    "    if next_next_token.strip() != '':\n",
    "        tokens_text += ' ' + next_next_token.strip()\n",
    "    if tokens_text in design_people:\n",
    "#         print(token, '           ', tokens_text)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "stop_words = []\n",
    "stop_words.extend(stop_words1)\n",
    "stop_words.extend(stop_words9)\n",
    "stop_words.extend(stop_words9M)\n",
    "stop_words.extend(stop_wordsU)\n",
    "stop_words.extend(stop_wordsF)\n",
    "stop_words.extend(stop_words10)\n",
    "\n",
    "stop_words.extend(stop_words2)\n",
    "stop_words.extend(stop_words3)\n",
    "\n",
    "if NAMES_SELECTION == 2:\n",
    "    stop_words.extend(stop_wordsN7)\n",
    "    stop_words.extend(stop_wordsY)\n",
    "\n",
    "if NAMES_SELECTION != None:\n",
    "    stop_wordsU = set(stop_wordsU) ##\n",
    "    stop_words = set(stop_words) ##\n",
    "    print('len stop_words: ', len(stop_words)) ##\n",
    "else:\n",
    "    stop_wordsU = None\n",
    "    stop_words = None\n",
    "\n",
    "\n",
    "def check_for_pii(tokens):\n",
    "    #return True #disable check\n",
    "    for i, token in enumerate(tokens):\n",
    "\n",
    "        if ('google.' in token or 'wikipedia.com' in token or 'trello.com' in token or 'miro.com' in token or 'coursera.org' in token or '.gov' in token):\n",
    "            continue\n",
    "            \n",
    "        if (i < len(tokens)-1 \n",
    "            and (token == 'Angela' and tokens[i+1] == 'Myer' or token == 'David' and tokens[i+1] == 'Gray' or token == 'Dave' and tokens[i+1] == 'Gray' \n",
    "                 or token == 'Tim' and tokens[i+1] == 'Brown' or token == 'Jeanne' and tokens[i+1] == 'Liedtka' or token == 'M.' and tokens[i+1] == 'Liedtka' \n",
    "                 or token == 'Jeanne' and tokens[i+1] == 'M.' \n",
    "                )\n",
    "            or token in {'Myer', 'Meyer', 'Brown', 'Gray', 'Lietdka'}):    \n",
    "            continue\n",
    "            \n",
    "        if NAMES_SELECTION != None:\n",
    "            if token in all_names and token[0].isupper() and token not in stop_words:\n",
    "                return True\n",
    "        \n",
    "        if NAMES_SELECTION == 2:\n",
    "            #N7 UU:\n",
    "            if token[0].isupper() and token.lower() not in all_lower_set and token.isalpha() and token[1:].islower() and token not in stop_wordsU and token not in stop_words and token not in all_names:\n",
    "                return True\n",
    "\n",
    "        if 'http' in token or 'www.' in token        or '@' in token:\n",
    "            return True\n",
    "            \n",
    "        if NAMES_SELECTION == None or NAMES_SELECTION == 2:\n",
    "            if token.lower() in {'pin', 'id', 'studentid', 'idstudent', 'pinno'} or token in {'NO', 'Roll', 'ROLL', 'Number', 'Interviewer', 'Interviewee', 'Author', 'AUTHOR'}:\n",
    "                return True\n",
    "\n",
    "        if NAMES_SELECTION == None:\n",
    "            if token[0].isupper() and token.lower() not in all_lower_set and token.isalpha() and token[1:].islower():\n",
    "                return True\n",
    "            if token in {'Final', 'Assignment', 'FINAL', 'ASSIGNMENT', 'VISUALIZATION', 'CHALLENGE', 'STORYTELLING', 'MINDMAPPING', 'SELECTION', 'DESIGN', 'REFLECTION', 'DESIGN', 'LAUNCH', 'STORY'}:\n",
    "                return True\n",
    "            if token in {'Challenge', 'Mapping', 'Reflection'}:\n",
    "                return True\n",
    "\n",
    "        if len(token)>=4 and token[-4:].isdigit() or token == 'ID':\n",
    "            return True\n",
    "        \n",
    "        if token == 'UVA':\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "df = df[df['tokens'].apply(check_for_pii)]\n",
    "print(len(df))\n",
    "stop = time.time()\n",
    "print('df filter: ', stop - start)\n",
    "train_dataset = Dataset.from_pandas(df, preserve_index=False)\n",
    "dataset = DatasetDict({'train': train_dataset})\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "dataset = dataset.map(tokenize_and_align_original_columns_new, batched=True, remove_columns=columns_to_remove, num_proc=3)\n",
    "print('dataset after map tokenize_and_align_original_columns_new function:')\n",
    "print(dataset)\n",
    "stop = time.time()\n",
    "print('dataset.map tokenize_and_align_original_columns_new processing time: ', stop - start)\n",
    "\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# inference_model_name = base_model\n",
    "start = time.time()\n",
    "model = AutoModelForTokenClassification.from_pretrained(inference_model_name, num_labels=len(labels), id2label=id2label, label2id=label2id)\n",
    "model20 = AutoModelForTokenClassification.from_pretrained(inference_model_name2, num_labels=len(labels), id2label=id2label, label2id=label2id)\n",
    "\n",
    "stop = time.time()\n",
    "print('models loading time: ', stop - start)\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='my_pii_model',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    metric_for_best_model='f5',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer20 = Trainer(\n",
    "    model=model20,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    tokenizer=tokenizer20,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "def eval_func(model, test_dataset):\n",
    "    # torch.set_num_threads(4)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, collate_fn=data_collator) #, num_workers=2)\n",
    "    test_loader_len = len(test_loader)\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "\n",
    "    sess_options = onnxruntime.SessionOptions()\n",
    "    sess_options.intra_op_num_threads = 1\n",
    "    session = onnxruntime.InferenceSession(model, providers=['CPUExecutionProvider'], sess_options=sess_options)\n",
    "    output_layer_name = 'logits'\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for _, batch in tqdm(enumerate(test_loader), total=test_loader_len):\n",
    "            input_data = {}\n",
    "            input_data['input_ids'] = batch['input_ids'].numpy()\n",
    "            input_data['attention_mask'] = batch['attention_mask'].numpy()\n",
    "            result = session.run([output_layer_name], input_data) #input_data)\n",
    "            results.append(result)\n",
    "\n",
    "        results = np.concatenate(results, axis=1)\n",
    "        results = np.squeeze(results, axis=0)\n",
    "    return results\n",
    "\n",
    "def predict_with_thread_pool(modelo, dataset):\n",
    "\n",
    "    dataset_with_columns_removed = dataset.remove_columns(['document', 'full_text', 'tokens', 'token_type_ids', 'overflow_to_sample_mapping', 'word_ids'])\n",
    "    dataset_1 = dataset_with_columns_removed.shard(num_shards=4, index=0, contiguous=True)\n",
    "    dataset_2 = dataset_with_columns_removed.shard(num_shards=4, index=1, contiguous=True)\n",
    "    dataset_3 = dataset_with_columns_removed.shard(num_shards=4, index=2, contiguous=True)\n",
    "    dataset_4 = dataset_with_columns_removed.shard(num_shards=4, index=3, contiguous=True)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor: #80s with 4 threads vs 100s with one\n",
    "        future = executor.submit(eval_func, modelo, dataset_1)\n",
    "        future2 = executor.submit(eval_func, modelo, dataset_2)\n",
    "        future3 = executor.submit(eval_func, modelo, dataset_3)\n",
    "        future4 = executor.submit(eval_func, modelo, dataset_4)\n",
    "\n",
    "        return_value = future.result()\n",
    "        return_value2 = future2.result()\n",
    "        return_value3 = future3.result()\n",
    "        return_value4 = future4.result()\n",
    "\n",
    "        predictions = np.concatenate((return_value, return_value2, return_value3, return_value4))\n",
    "        print('predictions.shape: ', predictions.shape)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "print('torch threads: ', torch.get_num_threads())\n",
    "torch.set_num_threads(4)\n",
    "print('torch threads: ', torch.get_num_threads())\n",
    "\n",
    "if USE_ONNX:\n",
    "    predictions = predict_with_thread_pool(onnx_model, dataset['train'])\n",
    "else:\n",
    "    predictions = trainer.predict(dataset['train']).predictions\n",
    "\n",
    "\n",
    "stop = time.time()\n",
    "print('inference part 1 - time elapsed: ', stop - start)\n",
    "\n",
    "\n",
    "################# nowe\n",
    "# import torch.nn.functional as F\n",
    "# predictions = torch.from_numpy(predictions)\n",
    "# predictions = F.log_softmax(predictions, dim=-1)\n",
    "# predictions = predictions.numpy()\n",
    "# print(predictions[0][0])\n",
    "# #predictions[:, :, 0] -= 885\n",
    "# predictions[:, :, 2] += 2 #B-ID_NUM\n",
    "# predictions[:, :, 7] += 2 #B-USERNAME\n",
    "# predictions[:, :, 8] += 2 #I-ID_NUM\n",
    "# print(predictions[0][0])\n",
    "#################\n",
    "\n",
    "start = time.time()\n",
    "print('processing predictions part 1...')\n",
    "predicted_token_class = np.argmax(predictions, axis=-1)\n",
    "predicted_token_score = np.max(predictions, axis=-1)\n",
    "# print(predicted_token_class)\n",
    "print(len(predicted_token_class)) #number of rows in ds\n",
    "print(len(predicted_token_class[0])) #512\n",
    "\n",
    "AVG_SCORE_THRESHOLD = 9998.5\n",
    "print('AVG_SCORE_THRESHOLD: ', AVG_SCORE_THRESHOLD)\n",
    "\n",
    "token_in_document_dict = {}\n",
    "csv_rows = []\n",
    "row_id = 0\n",
    "non_O_examples = []\n",
    "high_confidence_rows = []\n",
    "for i, (example, predicted, scores) in enumerate(zip(dataset['train'], predicted_token_class, predicted_token_score)):\n",
    "    final_labels = {}\n",
    "    previous_word_idx = example['word_ids'][0] #it is None\n",
    "    for word_idx, predicted_label in zip(example['word_ids'], predicted):\n",
    "        if word_idx != previous_word_idx and word_idx != None:\n",
    "            if predicted_label != 0:\n",
    "                final_labels[word_idx] = id2label[predicted_label]\n",
    "        previous_word_idx = word_idx\n",
    "        \n",
    "    scores = [score for score, label in zip(scores, predicted) if label != 0]\n",
    "    avg_score = sum(scores) / (len(scores) + 1e-8)\n",
    "\n",
    "    example_added = False\n",
    "    for token_id, final_label in final_labels.items():\n",
    "        if final_label != 'O':\n",
    "            document_id = example['document']\n",
    "            if avg_score > AVG_SCORE_THRESHOLD:\n",
    "                high_confidence_rows.append([row_id, document_id, token_id, final_label])\n",
    "            elif not example_added:\n",
    "                ###\n",
    "                token = document_to_rows[document_id]['tokens'][token_id]\n",
    "\n",
    "                if document_id not in token_in_document_dict:\n",
    "                    token_in_document_dict[document_id] = {}\n",
    "                if token not in token_in_document_dict[document_id]:\n",
    "                    token_in_document_dict[document_id][token] = 1\n",
    "                else:\n",
    "                    token_in_document_dict[document_id][token] += 1\n",
    "                    \n",
    "                if token_in_document_dict[document_id][token] <= 3:              \n",
    "                    non_O_examples.append(example)\n",
    "                    example_added = True\n",
    "                else:##\n",
    "                    print('token_in_document_dict more than 3 times: ', document_id, token)##\n",
    "\n",
    "            csv_rows.append([row_id, document_id, token_id, final_label])\n",
    "            row_id += 1\n",
    "stop = time.time()\n",
    "print('processing predictions part 1. time elapsed: ', stop - start)\n",
    "\n",
    "           \n",
    "print('len non_O_examples: ', len(non_O_examples))\n",
    "print('len high_confidence_rows: ', len(high_confidence_rows))\n",
    "\n",
    "if True:\n",
    "    dataset_non_O = Dataset.from_pandas(pd.DataFrame(non_O_examples))\n",
    "    # dataset_non_O = Dataset.from_list(non_O_examples)\n",
    "    print(dataset_non_O)\n",
    "\n",
    "    start = time.time()\n",
    "    if USE_ONNX:\n",
    "        predictions = predict_with_thread_pool(onnx_model20, dataset_non_O)\n",
    "    else:\n",
    "        predictions = trainer20.predict(dataset_non_O).predictions\n",
    "\n",
    "    stop = time.time()\n",
    "    print('dataset_non_O predict: ', stop - start)\n",
    "\n",
    "\n",
    "    ############################ copy\n",
    "    start = time.time()\n",
    "    ################# nowe phase 2\n",
    "#     import torch.nn.functional as F\n",
    "#     predictions = torch.from_numpy(predictions)\n",
    "#     predictions = F.log_softmax(predictions, dim=-1)\n",
    "#     predictions = predictions.numpy()\n",
    "#     print(predictions[0][0])\n",
    "# #     predictions[:, :, 0] -= 1\n",
    "#     predictions[:, :, 2] += 1 #B-ID_NUM\n",
    "# #     predictions[:, :, 7] += 6 #B-USERNAME\n",
    "#     predictions[:, :, 8] += 1 #I-ID_NUM\n",
    "#     predictions[:, :, 5] += 1 #B-ADDRESS\n",
    "#     predictions[:, :, 11] += 1 #I-ADDRESS\n",
    "#     print(predictions[0][0])\n",
    "    #################\n",
    "    predicted_token_class = np.argmax(predictions, axis=-1)\n",
    "    # print(predicted_token_class)\n",
    "    print(len(predicted_token_class)) #number of rows in ds\n",
    "    print(len(predicted_token_class[0])) #512\n",
    "\n",
    "    csv_rows = []\n",
    "    row_id = 0\n",
    "    non_O_examples = []\n",
    "    for i, (example, predicted) in enumerate(zip(dataset_non_O, predicted_token_class)):\n",
    "        final_labels = {}\n",
    "        previous_word_idx = example['word_ids'][0] #it is None\n",
    "        for word_idx, predicted_label in zip(example['word_ids'], predicted):\n",
    "            if word_idx != previous_word_idx and word_idx != None:\n",
    "                if predicted_label != 0: #added\n",
    "                    final_labels[word_idx] = id2label[predicted_label]\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        example_added = False\n",
    "        for token_id, final_label in final_labels.items():\n",
    "            if final_label != 'O':\n",
    "                if not example_added:\n",
    "                    non_O_examples.append(example)\n",
    "                    example_added = True\n",
    "                document_id = example['document']\n",
    "                csv_rows.append([row_id, document_id, token_id, final_label])\n",
    "                row_id += 1\n",
    "    stop = time.time()\n",
    "    print('len non_O_examples part 2: ', len(non_O_examples))\n",
    "    print('processing predictions part 2. time elapsed: ', stop - start)\n",
    "    ############################ copy\n",
    "\n",
    "\n",
    "    \n",
    "print('extending csv_rows with high_confidence_rows from part 1')\n",
    "csv_rows.extend(high_confidence_rows)\n",
    "#----------------------------------------------------------------------\n",
    "csv_rows.extend(csv_simple_rule_rows)\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "            \n",
    "\n",
    "def simple_unique_fix(csv_rows):\n",
    "    out_set = set()\n",
    "    out_list = []\n",
    "    for row in csv_rows:\n",
    "        t = (row[1], row[2], row[3])\n",
    "        if t not in out_set:\n",
    "            out_list.append(t)\n",
    "            out_set.add(t)\n",
    "        \n",
    "    print('after trivial unique fix: ', len(out_set), len(out_list))\n",
    "\n",
    "    out_dict = {}\n",
    "    out_list_cleaned = []\n",
    "    for t in out_list:\n",
    "        document = t[0]\n",
    "        token = t[1]\n",
    "        label = t[2]\n",
    "        if (document, token) in out_dict:\n",
    "            print('skipping duplicate label for: ', (document, token), out_dict[(document, token)], '  ignore:  ', label)\n",
    "        else:\n",
    "            out_dict[(document, token)] = label\n",
    "            \n",
    "    for k,v in out_dict.items():\n",
    "        out_list_cleaned.append((k[0], k[1], v))\n",
    "    out_list_cleaned.sort()\n",
    "        \n",
    "    print('after unique fix for inconsistent predictions: ', len(out_list_cleaned))\n",
    "            \n",
    "    csv_rows_cleaned = []\n",
    "    for i, t in enumerate(out_list_cleaned):\n",
    "        row = [i] + list(t)\n",
    "        csv_rows_cleaned.append(row)\n",
    "        # print(row)\n",
    "        \n",
    "    return csv_rows_cleaned\n",
    "    \n",
    "def fix_b_i_order(csv_rows):\n",
    "    previous_document = -1\n",
    "    previous_token = -999\n",
    "    previous_label = 'NOTHING'\n",
    "    for i in range(len(csv_rows)):\n",
    "        row = csv_rows[i]\n",
    "        t = (row[1], row[2], row[3])   \n",
    "        document = t[0]\n",
    "        token = t[1]\n",
    "        label = t[2]\n",
    "        if document == previous_document:\n",
    "            if token != previous_token + 1:\n",
    "                if 'I-' in label:\n",
    "                    print('fixing starting label from I- to B-: ', document, token, label)\n",
    "                    label = label.replace('I-', 'B-')\n",
    "                    csv_rows[i][3] = label\n",
    "                \n",
    "            if token == previous_token + 1:\n",
    "                if label[2:] == previous_label[2:]:\n",
    "                    if 'B-' in label:\n",
    "                        print('fixing continuation label from B- to I-: ', document, token, label)\n",
    "                        label = label.replace('B-', 'I-')\n",
    "                        csv_rows[i][3] = label\n",
    "        \n",
    "        if document != previous_document:\n",
    "            if 'I-' in label:\n",
    "                print('fixing first token occurence in document from I- to B-', document, token, label)\n",
    "                label = label.replace('I-', 'B-')\n",
    "                csv_rows[i][3] = label\n",
    "        \n",
    "        previous_document = document\n",
    "        previous_token = token\n",
    "        previous_label = label\n",
    "\n",
    "\n",
    "\n",
    "def fix_missed_tokens_by_stats(csv_rows):\n",
    "    print('fix_missed_tokens_by_stats: creating document to tokens dict:')\n",
    "    document_to_tokens= {}\n",
    "    for i, row in tqdm(original_df.iterrows()):\n",
    "        document_to_tokens[row['document']] = row['tokens']    \n",
    "    \n",
    "    document_to_tokens_count = {}\n",
    "    for csv_row in csv_rows:\n",
    "        document = csv_row[1]\n",
    "        token_idx = csv_row[2]\n",
    "        label = csv_row[3]\n",
    "        tokens = document_to_tokens[document]\n",
    "        token = tokens[token_idx]\n",
    "        if document not in document_to_tokens_count:\n",
    "            document_to_tokens_count[document] = {}\n",
    "            \n",
    "        if token not in document_to_tokens_count[document]:\n",
    "            document_to_tokens_count[document][token] = 0\n",
    "        else:\n",
    "            document_to_tokens_count[document][token] += 1\n",
    "            \n",
    "    csv_rows_new = []\n",
    "    already_added = set()\n",
    "    for csv_row in csv_rows:\n",
    "        document = csv_row[1]\n",
    "        token_idx = csv_row[2]\n",
    "        label = csv_row[3]\n",
    "        tokens = document_to_tokens[document]\n",
    "        token = tokens[token_idx]            \n",
    "        \n",
    "        all_token_count = tokens.count(token)\n",
    "        predicted_token_count = document_to_tokens_count[document][token]\n",
    "        if all_token_count > 2: #>2 - .96 >1: .957; >3 .96 (best); 4 .96 (worst)\n",
    "            if len(token.strip()) > 1: #1: >1: .96\n",
    "                if predicted_token_count / all_token_count > 0: #0.49:\n",
    "                    token_indexes = [i for i, t in enumerate(tokens) if t == token]\n",
    "                    for token_idx2 in token_indexes:\n",
    "                        csv_row_new = csv_row.copy()\n",
    "                        csv_row_new[2] = token_idx2\n",
    "                        csv_rows_new.append(csv_row_new)\n",
    "                        already_added.add((csv_row_new[1], csv_row_new[2]))\n",
    "                    continue\n",
    "\n",
    "        if (csv_row[1], csv_row[2]) in already_added:\n",
    "            continue\n",
    "        csv_rows_new.append(csv_row.copy())\n",
    "    return csv_rows_new\n",
    "\n",
    "def has_digit(s):\n",
    "    result = False\n",
    "    for c in s:\n",
    "        if c.isdigit():\n",
    "            result = True\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def fix_impossible_predictions(csv_rows):\n",
    "    \n",
    "    print('fix_impossible_predictions: creating document to tokens dict:')\n",
    "    document_to_tokens= {}\n",
    "    for i, row in tqdm(original_df.iterrows()):\n",
    "        document_to_tokens[row['document']] = row\n",
    "\n",
    "    csv_rows_new = []\n",
    "    idx = 0\n",
    "    for csv_row in csv_rows:\n",
    "        document = csv_row[1]\n",
    "        token_idx = csv_row[2]\n",
    "        label = csv_row[3]\n",
    "        tokens = document_to_tokens[document]['tokens']\n",
    "        token = tokens[token_idx]\n",
    "        \n",
    "        start_idx = token_idx - 2\n",
    "        if start_idx < 0:\n",
    "            start_idx = 0\n",
    "        stop_idx = token_idx + 2\n",
    "        short_text = ' '.join(tokens[start_idx:stop_idx]).replace('\\n', ' ')\n",
    "        i = token_idx ##\n",
    "        \n",
    "        csv_row_new = csv_row.copy()\n",
    "        if '@' in token:\n",
    "            print('fixing impossible tokens @ -> B-EMAIL', token)\n",
    "            csv_row_new[3] = 'B-EMAIL'\n",
    "        elif 'http' in token or 'www.' in token:\n",
    "            print('fixing impossible tokens http/www -> B-URL_PERSONAL', token)\n",
    "            csv_row_new[3] = 'B-URL_PERSONAL'\n",
    "            label = 'B-URL_PERSONAL'\n",
    "        elif 'NAME_STUDENT' in label and has_digit(token):\n",
    "            if token[-5:].isdigit():\n",
    "                print('fixing impossible tokens digit in NAME_STUDENT -> B-ID_NUM', token)\n",
    "                csv_row_new[3] = 'B-ID_NUM'\n",
    "            else:\n",
    "                print('fixing impossible tokens digit in NAME_STUDENT -> B-USERNAME', token)\n",
    "                csv_row_new[3] = 'B-USERNAME'\n",
    "        elif 'NAME_STUDENT' in label and token in ['Mrs.', 'Mr.', 'Dr.', 'Ph.D.', 'Ms.', 'Prof.', 'Mrs', 'Mr', 'Dr', 'PhD', 'Ms', 'Prof', '\\n\\n', '&', '/', ':', ',']:\n",
    "            print('fixing impossible token word', token)\n",
    "            csv_row_new = None\n",
    "        elif label == 'B-URL_PERSONAL' and 'http' not in token and 'www.' not in token:\n",
    "            print('fixing impossible B-URL_PERSONAL -> 0', token)\n",
    "            csv_row_new = None\n",
    "        \n",
    "#         elif NAMES_SELECTION == 2:\n",
    "#             if i < len(tokens)-1 and token == 'Will' and tokens[i+1] in {'I', 'Smith', 'apply', 'ask', 'be', 'focus', 'help', 'improve', 'increase', 'it', 'listen', 'look', 'make', 'our', 'permit', 'take', 'the', 'there', 'they', 'try', 'use', 'we', 'you', 'he', 'she'}:\n",
    "#                 print('fixing WILL -> 0: ', token, tokens[i+1])\n",
    "#                 csv_row_new = None\n",
    "#             elif token in {'Myer', 'Meyer', 'Lietdka'}:\n",
    "#                 print('fixing LECTURERS -> 0: ', token)\n",
    "#                 csv_row_new = None\n",
    "#             elif check_designers(tokens[i:i+3]):\n",
    "#                 print('fixing DESIGNERS -> 0: ', tokens[i:i+3])\n",
    "#                 csv_row_new = None\n",
    "#             elif i > 0 and check_designers(tokens[i-1:i+2]):\n",
    "#                 print('fixing DESIGNERS -> 0: ', tokens[i-1:i+2])\n",
    "#                 csv_row_new = None\n",
    "#             elif i > 1 and check_designers(tokens[i-2:i+1]):\n",
    "#                 print('fixing DESIGNERS -> 0: ', tokens[i-2:i+1])\n",
    "#                 csv_row_new = None\n",
    "        \n",
    "        if 'URL_PERSONAL' in label and ('google.' in token or 'wikipedia.com' in token or 'trello.com' in token or 'miro.com' in token or 'coursera.org' in token or '.gov' in token):\n",
    "            print('fixing impossible tokens FP -> 0 URL_PERSONAL false positive', token)\n",
    "            csv_row_new = None\n",
    "        \n",
    "        if csv_row_new is not None:\n",
    "            csv_row_new[0] = idx\n",
    "            idx += 1\n",
    "            csv_rows_new.append(csv_row_new)\n",
    "\n",
    "    return csv_rows_new\n",
    "\n",
    "\n",
    "csv_rows = fix_missed_tokens_by_stats(csv_rows)\n",
    "\n",
    "csv_rows = simple_unique_fix(csv_rows)\n",
    "\n",
    "csv_rows = fix_impossible_predictions(csv_rows)\n",
    "\n",
    "print('len(csv_rows): ', len(csv_rows))\n",
    "\n",
    "fix_b_i_order(csv_rows)\n",
    "print('len(csv_rows): ', len(csv_rows))\n",
    "\n",
    "\n",
    "csv_header = ['row_id', 'document', 'token', 'label']\n",
    "out_file = 'submission.csv'\n",
    "\n",
    "with open(out_file, 'w', encoding='utf-8', newline='\\n') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(csv_header)\n",
    "    csv_writer.writerows(csv_rows)\n",
    "print(f'output written to {out_file}')\n",
    "    \n",
    "total_stop = time.time()\n",
    "print('total time elapsed: ', total_stop - total_start)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b711753b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
